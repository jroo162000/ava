"""
AVA Standalone (Server-Assisted) with Deepgram ASR+TTS

This version removes OpenAI Realtime usage. It:
- Streams microphone PCM16 to Deepgram Live (ASR)
- Sends final transcripts to the local AVA server (/respond) for the brain/tools
- Synthesizes TTS via Deepgram Speak and plays audio locally
- Adds simple barge-in and echo gating
"""

import asyncio
import base64
import json
import os
import sys
import wave
import threading
import queue
from datetime import datetime
from pathlib import Path
import time
import urllib.request
import urllib.error
import ssl
import re
import platform
import subprocess

# Set UTF-8 encoding for Windows console
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        os.environ['PYTHONIOENCODING'] = 'utf-8'

import websockets
import pyaudio
from deepgram import DeepgramClient
from deepgram.core.events import EventType
from deepgram.extensions.types.sockets import (
    AgentV1Agent,
    AgentV1AudioConfig,
    AgentV1AudioInput,
    AgentV1AudioOutput,
    AgentV1DeepgramSpeakProvider,
    AgentV1Endpoint,
    AgentV1GoogleThinkProvider,
    AgentV1AnthropicThinkProvider,
    AgentV1OpenAiThinkProvider,
    AgentV1GroqThinkProvider,
    AgentV1Listen,
    AgentV1ListenProvider,
    AgentV1SettingsMessage,
    AgentV1SocketClientResponse,
    AgentV1SpeakProviderConfig,
    AgentV1Think,
)
from corrected_tool_definitions import CORRECTED_TOOLS

# Local voice fallback imports (Whisper + Edge TTS)
try:
    from faster_whisper import WhisperModel
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    print("[warning] faster-whisper not installed - local fallback unavailable")

try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
except ImportError:
    EDGE_TTS_AVAILABLE = False
    print("[warning] edge-tts not installed - local fallback unavailable")

LOCAL_FALLBACK_AVAILABLE = WHISPER_AVAILABLE and EDGE_TTS_AVAILABLE

# Personality system import
try:
    from ava_personality import get_personality, get_personality_context, get_greeting, get_acknowledgment
    PERSONALITY_AVAILABLE = True
except ImportError:
    PERSONALITY_AVAILABLE = False
    print("[warning] ava_personality not found - personality system unavailable")

# Self-modification system import
try:
    from ava_self_modification import (
        self_mod_tool_handler,
        CORE_FILES as SELF_MOD_CORE_FILES,
        CODING_KNOWLEDGE,
        diagnose_codebase,
        diagnose_error
    )
    SELF_MOD_AVAILABLE = True
except ImportError:
    SELF_MOD_AVAILABLE = False
    print("[warning] ava_self_modification not found - self-modification unavailable")

# Self-awareness system import
try:
    from ava_self_awareness import (
        get_self_awareness,
        introspect,
        who_am_i,
        diagnose as self_diagnose,
        get_prompt_context,
        learn_from_correction,
        check_past_mistakes
    )
    SELF_AWARENESS_AVAILABLE = True
except ImportError:
    SELF_AWARENESS_AVAILABLE = False
    print("[warning] ava_self_awareness not found - self-awareness unavailable")

# Passive learning system import
try:
    from ava_passive_learning import (
        get_passive_learning,
        start_passive_learning,
        stop_passive_learning,
        get_current_context as get_passive_context,
        record_interaction,
        get_learning_summary
    )
    PASSIVE_LEARNING_AVAILABLE = True
except ImportError:
    PASSIVE_LEARNING_AVAILABLE = False
    print("[warning] ava_passive_learning not found - passive learning unavailable")

# Add cmp-use to path
sys.path.insert(0, r"C:\Users\USER 1\cmp-use")

from cmpuse.secrets import load_into_env
from cmpuse.agent_core import Agent, Plan, Step
from cmpuse.config import Config
import cmpuse.tools

# Load secrets and configuration
load_into_env()

# Enable full access
os.environ['CMPUSE_ALLOW_SHELL'] = '1'
os.environ['CMPUSE_FORCE'] = '1'
os.environ['CMPUSE_CONFIRM'] = '0'
os.environ['CMPUSE_DRY_RUN'] = '0'
os.environ['CMPUSE_ALLOW_NETWORK'] = '1'
os.environ['CMPUSE_PATH_WHITELIST'] = "C:\\"

# Audio configuration
MIC_RATE = 16000           # Mic capture rate - MUST be 16kHz for Deepgram Agent
PLAYBACK_RATE = 24000      # TTS playback target
CHANNELS = 1
CHUNK_SIZE = 480           # ~30ms at 16kHz for low-latency streaming
CHUNK_SAMPLES = 480        # ~30ms @16kHz for low-latency streaming
FORMAT = pyaudio.paInt16

# Deepgram endpoints
DG_LISTEN_URL = (
    "wss://api.deepgram.com/v1/listen?encoding=linear16&sample_rate="
    f"{MIC_RATE}&channels=1&model=nova-2&smart_format=true"
)
DG_SPEAK_BASE = "https://api.deepgram.com/v1/speak?model=aura-2-andromeda-en"

class WavToPcmStripper:
    """Strips WAV header and forwards only PCM data bytes.

    Reset this between clips (e.g., on AgentAudioDone).
    Tolerates data chunk size of 0 by streaming until reset.
    """

    def __init__(self):
        self.reset()

    def reset(self):
        self._need_header = True
        self._buffer = bytearray()
        self._data_started = False

    def feed(self, data: bytes) -> bytes:
        if not data:
            return b''
        if not self._need_header:
            return data  # already in data section
        # accumulate header until we find 'data' chunk
        self._buffer.extend(data)
        # minimal parser: look for 'data' chunk start; then emit everything after its 8-byte header
        buf = self._buffer
        if len(buf) < 44:
            return b''
        # Check RIFF/WAVE
        if not (buf[0:4] == b'RIFF' and buf[8:12] == b'WAVE'):
            # Not WAV; pass-through to avoid stalling
            self._need_header = False
            return bytes(buf)
        # Search for 'data' chunk from byte 12 onwards
        i = 12
        while i + 8 <= len(buf):
            chunk_id = buf[i:i+4]
            chunk_size = int.from_bytes(buf[i+4:i+8], 'little', signed=False)
            if chunk_id == b'data':
                start = i + 8
                self._need_header = False
                self._data_started = True
                out = bytes(buf[start:])
                # Clear buffer to avoid growth
                self._buffer = bytearray()
                return out
            # advance to next chunk
            i += 8 + chunk_size
            if i > len(buf) + 4096:
                # safety; don't spin
                break
        # header incomplete; wait for more bytes
        return b''


# ==================== LOCAL VOICE ENGINE (Whisper + Edge TTS) ====================

class LocalVoiceEngine:
    """Fallback voice engine using local Whisper ASR + Edge TTS.
    
    Used when Deepgram quota is exhausted. Completely offline ASR,
    free unlimited TTS via Microsoft Edge's neural voices.
    """
    
    def __init__(self, parent, whisper_model="base", edge_voice="en-US-MichelleNeural"):
        self.parent = parent  # Reference to StandaloneRealtimeAVA
        self.whisper_model_name = whisper_model
        self.edge_voice = edge_voice
        self.whisper_model = None
        self.running = False
        self.shutdown = threading.Event()
        self._audio_buffer = bytearray()
        self._buffer_lock = threading.Lock()
        self._min_audio_length = 16000 * 2 * 1  # 1 second of 16kHz mono int16
        self._silence_threshold = 500  # RMS threshold for silence detection
        self._silence_duration = 0.8  # Seconds of silence before processing
        self._last_speech_time = 0
        
    def initialize(self):
        """Load Whisper model (done lazily to save memory)"""
        if not WHISPER_AVAILABLE:
            print("[local] Whisper not available")
            return False
        if self.whisper_model is None:
            print(f"[local] Loading Whisper model '{self.whisper_model_name}'...")
            try:
                # Use CPU by default, can change to "cuda" if GPU available
                self.whisper_model = WhisperModel(self.whisper_model_name, device="cpu", compute_type="int8")
                print(f"[local] Whisper model loaded successfully")
                return True
            except Exception as e:
                print(f"[local] Failed to load Whisper: {e}")
                return False
        return True
    
    def _rms(self, audio_bytes):
        """Calculate RMS of audio buffer"""
        if len(audio_bytes) < 2:
            return 0
        import struct
        n = len(audio_bytes) // 2
        samples = struct.unpack('<' + 'h' * n, audio_bytes[:n*2])
        return (sum(s*s for s in samples) / n) ** 0.5
    
    async def synthesize_speech(self, text):
        """Generate speech using Edge TTS and play it"""
        if not text or not EDGE_TTS_AVAILABLE:
            return
        
        try:
            print(f"[local-tts] Synthesizing: {text[:50]}...")
            communicate = edge_tts.Communicate(text, self.edge_voice)
            
            audio_data = bytearray()
            async for chunk in communicate.stream():
                if chunk["type"] == "audio":
                    audio_data.extend(chunk["data"])
            
            if not audio_data:
                print("[local-tts] No audio data received")
                return
                
            print(f"[local-tts] Got {len(audio_data)} bytes, playing...")
            
            # Save to temp file
            import tempfile
            tmp_path = os.path.join(tempfile.gettempdir(), f"ava_tts_{int(time.time()*1000)}.mp3")
            with open(tmp_path, 'wb') as f:
                f.write(audio_data)
            
            self.parent.tts_active.set()
            played = False
            
            # Method 1: Try pygame
            try:
                import pygame
                if not pygame.mixer.get_init():
                    pygame.mixer.init()
                
                pygame.mixer.music.load(tmp_path)
                pygame.mixer.music.play()
                
                # Wait with timeout based on audio size (rough estimate: 1 sec per 16KB)
                max_wait = max(5, len(audio_data) // 8000)
                start = time.time()
                
                while pygame.mixer.music.get_busy():
                    if self.parent.user_speaking.is_set() or self.shutdown.is_set():
                        pygame.mixer.music.stop()
                        print("[local-tts] Interrupted")
                        break
                    if time.time() - start > max_wait:
                        print("[local-tts] Playback timeout")
                        break
                    time.sleep(0.1)
                
                played = True
                print("[local-tts] Playback complete")
                
            except Exception as e:
                print(f"[local-tts] pygame failed: {e}")
            
            # Method 2: Fallback to Windows Media Player via subprocess
            if not played:
                try:
                    import subprocess
                    # Use Windows start command to play audio
                    subprocess.run(
                        ['cmd', '/c', 'start', '/wait', '', tmp_path],
                        shell=False, timeout=30
                    )
                    played = True
                except Exception as e:
                    print(f"[local-tts] subprocess failed: {e}")
            
            self.parent.tts_active.clear()
            
            # Cleanup
            try:
                time.sleep(0.5)
                os.unlink(tmp_path)
            except:
                pass
                        
        except Exception as e:
            print(f"[local-tts] Error: {e}")
            import traceback
            traceback.print_exc()
    
    def transcribe_audio(self, audio_bytes):
        """Transcribe audio using Whisper with hallucination filtering"""
        if not self.whisper_model or len(audio_bytes) < self._min_audio_length:
            return ""
        
        try:
            import numpy as np
            import struct
            
            # Convert bytes to numpy array
            n_samples = len(audio_bytes) // 2
            samples = struct.unpack('<' + 'h' * n_samples, audio_bytes)
            audio_np = np.array(samples, dtype=np.float32) / 32768.0
            
            # Check audio energy - skip if too quiet (likely noise/silence)
            rms = np.sqrt(np.mean(audio_np ** 2))
            if rms < 0.01:  # Very low energy, likely silence
                return ""
            
            # Transcribe
            segments, info = self.whisper_model.transcribe(audio_np, beam_size=5, language="en")
            text = " ".join([seg.text for seg in segments]).strip()
            
            # Filter out common Whisper hallucinations
            hallucination_patterns = [
                "thank you", "thanks for watching", "subscribe",
                "like and subscribe", "see you", "bye", "goodbye",
                "music", "applause", "[music]", "[applause]",
                "subtitles", "captions", "translated by",
                "hey bob", "my house", "that's my house",
                "www.", ".com", ".org",
            ]
            text_lower = text.lower()
            
            # If text is very short and matches hallucination patterns, skip
            if len(text) < 30:
                for pattern in hallucination_patterns:
                    if pattern in text_lower:
                        print(f"[local-asr] Filtered hallucination: {text}")
                        return ""
            
            # Skip if text doesn't match audio characteristics
            # (e.g., very long text from short audio is suspicious)
            audio_duration = len(audio_np) / 16000  # seconds
            words = len(text.split())
            words_per_second = words / max(audio_duration, 0.1)
            
            # Normal speech is 2-4 words per second, hallucinations often have many more
            if words_per_second > 6 and audio_duration < 2:
                print(f"[local-asr] Filtered suspicious speed ({words_per_second:.1f} w/s): {text}")
                return ""
            
            return text
        except Exception as e:
            print(f"[local-asr] Transcription error: {e}")
            return ""
    
    def run(self):
        """Main loop for local voice engine"""
        if not self.initialize():
            print("[local] Cannot start - Whisper initialization failed")
            return
        
        print("[local] ðŸŽ¤ Local voice engine started (Whisper + Edge TTS)")
        self.running = True
        self.shutdown.clear()
        
        p = pyaudio.PyAudio()
        
        # Open microphone
        in_kwargs = dict(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=1600)
        if self.parent.input_device_index is not None:
            in_kwargs['input_device_index'] = self.parent.input_device_index
        
        try:
            mic_stream = p.open(**in_kwargs)
        except Exception as e:
            print(f"[local] Mic open error: {e}")
            return
        
        print("[local] ðŸŽ¤ Microphone active - listening...")
        
        try:
            while not self.shutdown.is_set() and self.parent.running:
                try:
                    # Read audio chunk
                    audio_data = mic_stream.read(1600, exception_on_overflow=False)
                    rms = self._rms(audio_data)
                    now = time.time()
                    
                    # Skip if TTS is playing
                    if self.parent.tts_active.is_set():
                        time.sleep(0.01)
                        continue
                    
                    # Accumulate audio
                    with self._buffer_lock:
                        self._audio_buffer.extend(audio_data)
                    
                    # Detect speech/silence
                    if rms > self._silence_threshold:
                        self._last_speech_time = now
                    
                    # Process when silence detected after speech
                    buffer_duration = len(self._audio_buffer) / (16000 * 2)  # seconds
                    silence_elapsed = now - self._last_speech_time if self._last_speech_time > 0 else 0
                    
                    if buffer_duration > 0.5 and silence_elapsed > self._silence_duration:
                        with self._buffer_lock:
                            audio_to_process = bytes(self._audio_buffer)
                            self._audio_buffer.clear()
                        
                        self._last_speech_time = 0
                        
                        # Transcribe
                        transcript = self.transcribe_audio(audio_to_process)
                        if transcript:
                            print(f"\nðŸ—£ï¸  You: {transcript}")
                            
                            # Get response from server
                            loop = asyncio.new_event_loop()
                            try:
                                # Check if this is a correction
                                if hasattr(self.parent, '_detect_correction') and self.parent._detect_correction(transcript):
                                    self.parent._handle_correction(transcript)
                                
                                # Handle local intents first
                                handled = loop.run_until_complete(
                                    self.parent._maybe_handle_local_intent(transcript)
                                )
                                if not handled:
                                    # Check for past mistakes
                                    enhanced = transcript
                                    if hasattr(self.parent, '_get_enhanced_transcript'):
                                        enhanced = self.parent._get_enhanced_transcript(transcript)
                                    
                                    # Get server response
                                    reply = loop.run_until_complete(
                                        self.parent._ask_server_respond(enhanced)
                                    )
                                    if reply:
                                        print(f"ðŸ¤– AVA: {reply}")
                                        loop.run_until_complete(
                                            self.synthesize_speech(reply)
                                        )
                                        
                                        # Track for correction detection
                                        self.parent._last_user_transcript = transcript
                                        self.parent._last_ava_response = reply
                                        
                                        # Record interaction for passive learning
                                        if PASSIVE_LEARNING_AVAILABLE and hasattr(self.parent, 'passive_learning_enabled') and self.parent.passive_learning_enabled:
                                            try:
                                                record_interaction(transcript, reply, True)
                                            except:
                                                pass
                            finally:
                                loop.close()
                    
                    # Limit buffer size (max 30 seconds)
                    max_buffer = 16000 * 2 * 30
                    with self._buffer_lock:
                        if len(self._audio_buffer) > max_buffer:
                            self._audio_buffer = self._audio_buffer[-max_buffer:]
                            
                except Exception as e:
                    print(f"[local] Loop error: {e}")
                    time.sleep(0.1)
                    
        finally:
            self.running = False
            try:
                mic_stream.stop_stream()
                mic_stream.close()
            except:
                pass
            p.terminate()
            print("[local] Local voice engine stopped")
    
    def stop(self):
        """Stop the local voice engine"""
        self.shutdown.set()
        self.running = False


# Voice Engine State
class VoiceEngineState:
    """Tracks which voice engine is active and manages switching"""
    DEEPGRAM = "deepgram"
    LOCAL = "local"
    SWITCHING = "switching"
    
    def __init__(self):
        self.current = self.DEEPGRAM
        self.lock = threading.Lock()
        self.deepgram_available = True
        self.last_deepgram_check = 0
        self.deepgram_check_interval = 300  # Check every 5 minutes
        self.consecutive_errors = 0
        self.error_threshold = 3  # Switch after 3 consecutive errors


class StandaloneRealtimeAVA:
    def __init__(self):
        load_into_env()
        self.deepgram_key = os.getenv("DEEPGRAM_API_KEY") or self._read_key_file("deepgram key.txt")
        if not self.deepgram_key:
            raise ValueError("DEEPGRAM_API_KEY not found (set env or provide 'deepgram key.txt')")

        # Load LLM API keys for think providers (with fallback support)
        self.gemini_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY") or self._read_key_file("gemini api key.txt")
        self.claude_key = os.getenv("ANTHROPIC_API_KEY") or os.getenv("CLAUDE_API_KEY") or self._read_key_file("claude api key.txt")
        self.openai_key = os.getenv("OPENAI_API_KEY") or self._read_key_file("openai api key.txt")
        self.groq_key = os.getenv("GROQ_API_KEY") or self._read_key_file("groq api key.txt")

        # At least one think provider API key must be available
        if not any([self.gemini_key, self.claude_key, self.openai_key, self.groq_key]):
            raise ValueError("No LLM API key found. Need at least one of: Gemini, Claude, OpenAI, or Groq")

        # Set environment variables for available providers
        if self.gemini_key:
            os.environ['GOOGLE_API_KEY'] = self.gemini_key
        if self.claude_key:
            os.environ['ANTHROPIC_API_KEY'] = self.claude_key
        if self.openai_key:
            os.environ['OPENAI_API_KEY'] = self.openai_key
        if self.groq_key:
            os.environ['GROQ_API_KEY'] = self.groq_key

        self.config = Config.from_env()
        self.agent = Agent(self.config)

        # Audio setup
        self.audio = pyaudio.PyAudio()
        self.websocket = None  # unused now; kept for cleanup compatibility
        self.asr_ws = None
        self.running = False

        # Session state
        self.session_id = None

        # Audio playback queue and thread
        self.audio_queue = queue.Queue()
        self.playback_thread = None
        self.playback_stream = None
        # Barge-in / echo gating
        self.tts_active = threading.Event()
        self.user_speaking = threading.Event()
        self.barge_mode = threading.Event()
        # Per-turn suppression of Agent TTS when executing tools to avoid the agent speaking tool names/JSON
        self._drop_agent_tts = threading.Event()
        self._drop_until_ts = 0.0
        self.playback_busy = threading.Event()
        self._last_user_voice_t = 0.0
        self._loud_frames = 0
        self.START_THRESH = 1600  # ~ -26 dBFS (int16 RMS) - raised to reduce false barge-ins
        self.STOP_THRESH = 900    # ~ -29 dBFS - raised to reduce speaker bleed triggers
        self.SPEECH_HOLD_SEC = 0.6
        self.playback_rate = PLAYBACK_RATE
        self.output_device_index = None
        self.input_device_index = None
        # EMA of playback RMS to help echo gating across threads
        self._playback_rms_ema = 0.0
        # Debug flags (hotâ€‘reloadable via config)
        self.debug_agent = False
        self.debug_tools = False
        self._debug_log_path = str((Path(__file__).with_name('ava_debug.log')).resolve())

        # Correction tracking for pattern learning
        self._last_user_transcript = ""
        self._last_ava_response = ""
        self._correction_patterns = [
            r"^no[,.]?\s",
            r"^that'?s (not|wrong)",
            r"^i (said|meant|asked)",
            r"^actually[,.]?\s",
            r"^not what i",
            r"^wrong[,.]",
            r"^i didn'?t (say|mean|ask)",
            r"^you misunderstood",
            r"^that'?s not (right|correct|what)",
        ]

        # Hot-reloadable runtime config
        self.config_path = Path(__file__).with_name('ava_voice_config.json')
        self.cfg = {
            "speak_symbols": False,                 # if False: strip symbols/punctuation from TTS
            "server_url": "http://127.0.0.1:5051/respond",
            "server_route": "respond",  # 'chat' or 'respond'
            "vad": {"start_rms": self.START_THRESH, "stop_rms": self.STOP_THRESH, "hold_sec": self.SPEECH_HOLD_SEC},
            "audio": {"playback_rate": self.playback_rate, "output_device": None, "input_device": None},
            "asr_model": "nova-2",
            "tts_model": "aura-2-andromeda-en",
            "deepgram_api_key": None,
            "debug_asr": False,
            "debug_rms": False,
            "debug_text": False,
            "allow_barge": True,
            "voice_mode": "agent",
            "auto_start_server": True,
            "barge": {"min_tts_ms": 900, "debounce_frames": 6, "dyn_thresh_scale": 0.9},
            "local_fallback": {
                "whisper_model": "small",
                "edge_voice": "en-US-MichelleNeural",
                "auto_switch": True,
                "force_local": False,  # Set to true to skip Deepgram and use local only
                "health_check_interval": 300
            }
        }
        self._cfg_mtime = 0.0
        self._identity_mtime = 0.0
        self._load_config(silent=True)
        try:
            if self.identity_path.exists():
                self._identity_mtime = self.identity_path.stat().st_mtime
        except Exception:
            self._identity_mtime = 0.0

        # Identity profile
        self.identity_path = Path(__file__).with_name('ava_identity.json')
        self.identity = self._load_identity()
        self.started_at = time.time()
        self.metrics = {
            'asr_messages': 0,
            'asr_finals': 0,
            'tts_utterances': 0,
            'reconnects': 0,
            'last_error': '',
        }

        # Voice Engine State (auto-fallback between Deepgram and Local)
        self.voice_engine_state = VoiceEngineState()
        self.local_voice_engine = None
        if LOCAL_FALLBACK_AVAILABLE:
            local_cfg = self.cfg.get('local_fallback', {})
            self.local_voice_engine = LocalVoiceEngine(
                self,
                whisper_model=local_cfg.get('whisper_model', 'base'),
                edge_voice=local_cfg.get('edge_voice', 'en-US-MichelleNeural')
            )
            print("[voice] Local fallback engine available (Whisper + Edge TTS)")
        else:
            print("[voice] Local fallback NOT available (install faster-whisper and edge-tts)")

        # Personality system
        self.personality = None
        if PERSONALITY_AVAILABLE:
            try:
                self.personality = get_personality()
                print("[personality] Personality system loaded")
            except Exception as e:
                print(f"[personality] Error loading: {e}")
        else:
            print("[personality] Personality system NOT available")

        # Self-modification system
        self.self_mod_enabled = False
        if SELF_MOD_AVAILABLE:
            try:
                # Verify we can diagnose the codebase
                diag = diagnose_codebase()
                if diag.get("status") == "ok":
                    self.self_mod_enabled = True
                    print(f"[self-mod] Self-modification system loaded ({diag.get('files_found', 0)} core files)")
                else:
                    print(f"[self-mod] Codebase diagnosis failed: {diag.get('error', 'unknown')}")
            except Exception as e:
                print(f"[self-mod] Error initializing: {e}")
        else:
            print("[self-mod] Self-modification system NOT available")

        # Self-awareness system
        self.self_awareness = None
        self.self_awareness_enabled = False
        if SELF_AWARENESS_AVAILABLE:
            try:
                self.self_awareness = get_self_awareness()
                diag = self_diagnose()
                self.self_awareness_enabled = True
                status = diag.get("overall_status", "unknown")
                facts_count = diag.get("learning", {}).get("facts_learned", 0)
                print(f"[self-awareness] Self-awareness loaded (status: {status}, {facts_count} facts learned)")
            except Exception as e:
                print(f"[self-awareness] Error initializing: {e}")
        else:
            print("[self-awareness] Self-awareness system NOT available")

        # Passive learning system
        self.passive_learning = None
        self.passive_learning_enabled = False
        if PASSIVE_LEARNING_AVAILABLE:
            try:
                self.passive_learning = get_passive_learning()
                start_passive_learning()
                self.passive_learning_enabled = True
                summary = get_learning_summary()
                print(f"[passive-learning] Passive learning started ({summary.get('total_observations', 0)} observations)")
            except Exception as e:
                print(f"[passive-learning] Error initializing: {e}")
        else:
            print("[passive-learning] Passive learning NOT available")

        print("=" * 80)
        print("AVA STANDALONE - DEEPGRAM VOICE (Server-Assisted)")
        print("=" * 80)
        print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ASR: Deepgram nova-2 @ {MIC_RATE} Hz | TTS: aura-2-andromeda-en @ {PLAYBACK_RATE} Hz")
        print(f"Intelligence: AVA Server / Agent (no OpenAI Realtime)")
        print(f"Mode: ALWAYS-ON with Deepgram ASR+TTS")
        print(f"Tools Available: 20 JARVIS-level capabilities")
        print("=" * 80)
        print("Features:")
        print("  - Always listening (no wake word needed)")
        print("  - Bidirectional realtime voice conversation")
        print("  - Sub-second response latency")
        print("  - Can interrupt AVA mid-sentence")
        print("  - Full access to all 20 AVA tools")
        print("  - Smart Voice Activity Detection")
        print("=" * 80)

    def _dbg(self, tag: str, msg: str, data: dict | None = None):
        try:
            if not (self.debug_agent or self.debug_tools):
                return
            ts = time.strftime('%H:%M:%S')
            s = f"[DBG {tag} {ts}] {msg}"
            if data:
                try:
                    import json as _json
                    snippet = _json.dumps(data, ensure_ascii=False)
                    if len(snippet) > 300:
                        snippet = snippet[:300] + 'â€¦'
                    s += f" | {snippet}"
                except Exception:
                    pass
            print(s)
            try:
                with open(self._debug_log_path, 'a', encoding='utf-8') as lf:
                    lf.write(s + "\n")
            except Exception:
                pass
        except Exception:
            pass

    def get_tool_definitions(self):
        """Get tool definitions for function calling during voice chat - CORRECTED ACTIONS"""
        # TEMPORARY: Test with NO tools
        # return []
        from corrected_tool_definitions import CORRECTED_TOOLS
        return CORRECTED_TOOLS

    def _desktop_path(self) -> str:
        try:
            return str((Path.home() / 'Desktop').resolve())
        except Exception:
            return str(Path.home())

    def _try_tool_dispatch(self, text: str) -> str | None:
        """Lightweight intent â†’ tool router using corrected tools via cmpuse Agent.
        Returns a user-facing reply if a tool was executed, else None.
        """
        t = (text or '').strip()
        low = t.lower()
        try:
            # Create/make a file intent
            if ('create' in low or 'make' in low) and 'file' in low:
                name = None
                content = None
                m_name = re.search(r"named\s+([\w\-. ]+?)(?:\s+(?:that|with|containing)|$)", t, re.IGNORECASE)
                if m_name:
                    name = m_name.group(1).strip()
                m_content = re.search(r"(?:that|with|containing)\s+(?:says|say|text(?:\s+of)?|content)\s+(.+)$", t, re.IGNORECASE)
                if m_content:
                    content = m_content.group(1).strip()
                if not content:
                    # Fallback: use the whole request as content
                    content = t
                if not name:
                    name = f"ava_note_{int(time.time())}.txt"
                # Default to Desktop
                full_path = str(Path(self._desktop_path()) / name)
                res = asyncio.run(self.handle_tool_call('fs_ops', {
                    'operation': 'write',
                    'path': full_path,
                    'content': content
                }))
                if isinstance(res, dict) and res.get('status') == 'ok':
                    return f"I created the file {full_path}."
                return f"I tried to create {full_path} but something went wrong."

            # Remember that ... â†’ memory store
            if low.startswith('remember that '):
                value = t[len('remember that '):].strip()
                if value:
                    res = asyncio.run(self.handle_tool_call('memory_system', {
                        'action': 'store',
                        'key': f'note_{int(time.time())}',
                        'value': value
                    }))
                    return "Got it. I stored that in memory." if isinstance(res, dict) else "Stored."

            # Send email to ... subject ... body ... (very basic)
            if ('email' in low or 'send an email' in low) and ' to ' in low:
                to = None; subject = None; body = None
                m_to = re.search(r"to\s+([\w\-.+@]+)", t, re.IGNORECASE)
                if m_to: to = m_to.group(1)
                m_sub = re.search(r"subject\s*[:\-]\s*(.+?)(?:\s+body\s*[:\-]|$)", t, re.IGNORECASE)
                if m_sub: subject = m_sub.group(1).strip()
                m_body = re.search(r"body\s*[:\-]\s*(.+)$", t, re.IGNORECASE)
                if m_body: body = m_body.group(1).strip()
                if to and (subject or body):
                    res = asyncio.run(self.handle_tool_call('comm_ops', {
                        'action': 'send_email', 'to': to, 'subject': subject or '', 'body': body or ''
                    }))
                    return "Email sent." if isinstance(res, dict) else "Sent."

            # Open/navigate browser
            if ('open' in low or 'navigate' in low or 'go to' in low) and ('http://' in low or 'https://' in low or ' www.' in low or 'browser' in low):
                m_url = re.search(r"(https?://\S+|www\.[^\s]+)", t, re.IGNORECASE)
                url = m_url.group(1) if m_url else None
                if url and url.startswith('www.'): url = 'https://' + url
                res = asyncio.run(self.handle_tool_call('browser_automation', {
                    'action': 'launch'
                }))
                if url:
                    _ = asyncio.run(self.handle_tool_call('browser_automation', {
                        'action': 'navigate', 'url': url
                    }))
                return f"Opening the browser{(' to ' + url) if url else ''}."

            # Turn on/off lights/devices
            if ('turn on' in low or 'turn off' in low) and ('light' in low or 'lights' in low or 'device' in low):
                action = 'turn_on' if 'turn on' in low else 'turn_off'
                # crude room extraction
                m_room = re.search(r"in the ([a-zA-Z0-9 _-]+)", t, re.IGNORECASE)
                room = m_room.group(1).strip() if m_room else None
                res = asyncio.run(self.handle_tool_call('iot_ops', {
                    'action': action,
                    'room': room or ''
                }))
                return f"Okay, {action.replace('_',' ')} the lights{(' in ' + room) if room else ''}."

            # System info
            if 'system info' in low or 'computer info' in low or 'device info' in low:
                res = asyncio.run(self.handle_tool_call('sys_ops', { 'action': 'get_info' }))
                return "Here is the system information." if isinstance(res, dict) else "Done."

            # HTTP get
            if low.startswith('fetch ') or low.startswith('get ') or 'http' in low:
                m = re.search(r"(https?://\S+)", t)
                if m:
                    url = m.group(1)
                    res = asyncio.run(self.handle_tool_call('net_ops', { 'url': url }))
                    return f"Fetched {url}." if isinstance(res, dict) else "Fetched."

            # Calendar create event (very basic)
            if ('calendar' in low or 'event' in low) and ('create' in low or 'add' in low):
                m_sum = re.search(r"(?:event|calendar)\s*(?:called|named|for)?\s*([\w \-]{3,100})", t, re.IGNORECASE)
                summary = m_sum.group(1).strip() if m_sum else 'New Event'
                res = asyncio.run(self.handle_tool_call('calendar_ops', { 'action': 'create_event', 'summary': summary }))
                return "Event created." if isinstance(res, dict) else "Done."

            # Window list/focus
            if 'list windows' in low or 'what windows' in low:
                res = asyncio.run(self.handle_tool_call('window_ops', { 'action': 'list' }))
                return "Listing windows." if isinstance(res, dict) else "Done."
            if 'focus' in low and ('window' in low or 'app' in low):
                m_app = re.search(r"focus\s+(.*)$", t, re.IGNORECASE)
                app = (m_app.group(1).strip() if m_app else '')
                res = asyncio.run(self.handle_tool_call('window_ops', { 'action': 'focus', 'app': app }))
                return f"Focusing {app}." if app else "Focusing the window."

            # Camera capture
            if 'camera' in low and ('capture' in low or 'take a picture' in low):
                save_path = str(Path(self._desktop_path()) / f"ava_capture_{int(time.time())}.png")
                res = asyncio.run(self.handle_tool_call('camera_ops', { 'action': 'capture', 'save_path': save_path }))
                return f"Captured an image to {save_path}." if isinstance(res, dict) else "Captured."

            # Generic fallback: detect tool names and actions from corrected tools
            try:
                tools = self.get_tool_definitions()
                for td in tools:
                    name = str(td.get('name','')).strip()
                    if not name:
                        continue
                    # If the user explicitly mentions the tool name or a dotted alias, try it
                    if name in low or low.replace(' ', '_').find(name) >= 0:
                        params = td.get('parameters') or {}
                        props = (params.get('properties') or {})
                        args = {}
                        # Try to infer 'action' from enum by substring match
                        act = None
                        if isinstance(props.get('action',{}).get('enum',[]), list):
                            for a in props['action']['enum']:
                                if isinstance(a, str) and a in low:
                                    act = a; break
                        if act:
                            args['action'] = act
                        # Common parameter heuristics
                        # path/url/text/query/room/entity_id/brightness/temperature
                        m = re.search(r"(https?://\S+)", t)
                        if m and 'url' in props:
                            args['url'] = m.group(1)
                        m = re.search(r"(?:file|path)\s*[:\-]?\s*(\S+)", t, re.IGNORECASE)
                        if m and 'path' in props:
                            args['path'] = m.group(1)
                        m = re.search(r"(?:text|content)\s*[:\-]?\s*(.+)$", t, re.IGNORECASE)
                        if m and 'content' in props:
                            args['content'] = m.group(1)
                        m = re.search(r"room\s*[:\-]?\s*([\w\s-]+)", t, re.IGNORECASE)
                        if m and 'room' in props:
                            args['room'] = m.group(1).strip()
                        m = re.search(r"brightness\s*[:\-]?\s*(\d+)", t, re.IGNORECASE)
                        if m and 'brightness' in props:
                            args['brightness'] = int(m.group(1))
                        m = re.search(r"temperature\s*[:\-]?\s*(\d+(?:\.\d+)?)", t, re.IGNORECASE)
                        if m and 'temperature' in props:
                            args['temperature'] = float(m.group(1))
                        # Run tool if we have at least an action or common args
                        if args or 'action' in props:
                            res = asyncio.run(self.handle_tool_call(name, args))
                            if isinstance(res, dict):
                                msg = res.get('message') or res.get('status') or 'Done.'
                                return msg
            except Exception:
                pass
        except Exception:
            pass
        return None

    def _extract_tool_call(self, content: str):
        """Try to extract a tool call JSON from assistant text. Returns (name, args) or (None,None)."""
        try:
            if not content:
                return (None, None)
            s = content.strip()
            # Try fenced JSON
            m = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", s, re.IGNORECASE)
            if m:
                s = m.group(1)
            # If still not a pure JSON object, try to locate a {"tool":{...}} object substring
            if not (s.startswith('{') and s.endswith('}')):
                m2 = re.search(r"(\{\s*\"tool\"\s*:\s*\{[\s\S]*?\}\s*\})", s)
                if m2:
                    s = m2.group(1)
            # Parse JSON object (best effort)
            j = json.loads(s)
            if isinstance(j, dict):
                if 'tool' in j and isinstance(j['tool'], dict):
                    name = j['tool'].get('name')
                    args = j['tool'].get('arguments') or {}
                    if isinstance(name, str):
                        # Normalize tool name via synonyms
                        name = self._normalize_tool_name(name)
                        return (name, args if isinstance(args, dict) else {})
                # Alternate keys
                name = j.get('tool_name') or j.get('name')
                args = j.get('tool_args') or j.get('arguments') or {}
                if isinstance(name, str):
                    name = self._normalize_tool_name(name)
                    return (name, args if isinstance(args, dict) else {})
        except Exception:
            pass
        return (None, None)

    def _normalize_tool_name(self, name: str) -> str:
        n = (name or '').strip().lower()
        synonyms = {
            'file_write':'fs_ops','file_ops':'fs_ops','filegen':'fs_ops','file_gen':'fs_ops','filesystem':'fs_ops',
            'email':'comm_ops','gmail':'comm_ops','sms':'comm_ops','communications':'comm_ops','comm':'comm_ops',
            'http':'net_ops','fetch':'net_ops','network':'net_ops','net':'net_ops',
            'system':'sys_ops','sysinfo':'sys_ops','system_info':'sys_ops',
            'browser':'browser_automation','web_automation':'browser_automation','web':'browser_automation',
            'lights':'iot_ops','home':'iot_ops','mqtt':'iot_ops','iot':'iot_ops',
            'camera':'camera_ops','vision':'vision_ops','ocr':'vision_ops','screen':'screen_ops',
            'calendar':'calendar_ops','schedule':'calendar_ops',
            'windows':'window_ops','window':'window_ops',
            'mouse':'mouse_ops','keyboard':'key_ops','keys':'key_ops',
            'learning':'learning_db','memory':'memory_system','analysis':'analysis_ops','security':'security_ops',
            'remote':'remote_ops'
        }
        return synonyms.get(n, n)

    async def handle_tool_call(self, function_name, arguments):
        """Execute AVA tool calls during voice conversation"""
        print(f"\nðŸ”§ Tool call: {function_name}({arguments})")

        try:
            # Add default provider for email
            if function_name == "comm_ops" and arguments.get("action") == "send_email":
                arguments.setdefault("provider", "gmail")

            # Execute tool through AVA Agent
            plan = Plan(steps=[Step(tool=function_name, args={
                **arguments,
                "confirm": True
            })])

            results = self.agent.run(plan, force=True)

            # Process results
            if results and len(results) > 0:
                result = results[0]
                status = result.get('status', 'unknown')

                if status == 'ok':
                    return {
                        "status": "ok",
                        "message": result.get('message', 'Operation completed'),
                        "data": {k: v for k, v in result.items() if k not in ['status', 'message']}
                    }
                elif status == 'error':
                    return {
                        "status": "error",
                        "message": result.get('message', 'Operation failed'),
                        "note": result.get('note', '')
                    }
                elif status == 'info':
                    return {
                        "status": "info",
                        "message": result.get('message', ''),
                        "note": result.get('note', '')
                    }
                else:
                    return result
            else:
                return {"error": f"No results returned from {function_name}"}

        except Exception as e:
            return {
                "status": "error",
                "message": f"Tool execution error: {str(e)}",
                "tool": function_name
            }

    # ---------------------- Deepgram + Helpers ----------------------
    def _read_key_file(self, filename: str) -> str:
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return f.read().strip()
        except FileNotFoundError:
            return ''

    def _get_available_think_providers(self):
        """Returns list of available think providers in priority order.

        Priority: Gemini â†’ Claude â†’ Groq â†’ OpenAI
        Returns list of tuples: (provider_name, provider_class, model_name)
        """
        providers = []

        # Use Gemini first - matches avas_voice.py which works perfectly
        if self.gemini_key:
            providers.append(("Gemini", AgentV1GoogleThinkProvider, "gemini-2.5-flash"))

        if self.claude_key:
            # Deepgram supports Claude via Anthropic provider (has issues with cutoffs)
            providers.append(("Claude", AgentV1AnthropicThinkProvider, "claude-sonnet-4-20250514"))

        if self.groq_key:
            # Groq typically runs Llama models
            providers.append(("Groq", AgentV1GroqThinkProvider, "llama-3.3-70b-versatile"))

        if self.openai_key:
            providers.append(("OpenAI", AgentV1OpenAiThinkProvider, "gpt-4o"))

        return providers

    def _rms_int16(self, frame: bytes) -> float:
        if not frame:
            return 0.0
        n = len(frame) // 2
        if n <= 0:
            return 0.0
        import struct, math
        samples = struct.unpack('<' + 'h' * n, frame)
        acc = 0.0
        for s in samples:
            acc += s * s
        return math.sqrt(acc / n)

    def _cancel_tts(self):
        try:
            while not self.audio_queue.empty():
                _ = self.audio_queue.get_nowait()
        except Exception:
            pass

    def _load_config(self, silent: bool = False):
        try:
            if self.config_path.exists():
                st = self.config_path.stat()
                if st.st_mtime != self._cfg_mtime:
                    with open(self.config_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    if isinstance(data, dict):
                        self.cfg.update(data)
                        vad = self.cfg.get('vad') or {}
                        self.START_THRESH = int(vad.get('start_rms', self.START_THRESH))
                        self.STOP_THRESH = int(vad.get('stop_rms', self.STOP_THRESH))
                        self.SPEECH_HOLD_SEC = float(vad.get('hold_sec', self.SPEECH_HOLD_SEC))
                        # Audio updates
                        aud = self.cfg.get('audio') or {}
                        try:
                            pr = int(aud.get('playback_rate', self.playback_rate) or self.playback_rate)
                        except Exception:
                            pr = self.playback_rate
                        if pr != self.playback_rate:
                            self.playback_rate = pr
                            setattr(self, '_reopen_playback', True)
                        odi = aud.get('output_device')
                        if odi is not None and odi != self.output_device_index:
                            try:
                                self.output_device_index = int(odi)
                            except Exception:
                                self.output_device_index = odi
                            setattr(self, '_reopen_playback', True)
                        idi = aud.get('input_device')
                        if idi is not None and idi != self.input_device_index:
                            try:
                                self.input_device_index = int(idi)
                            except Exception:
                                self.input_device_index = idi
                            setattr(self, '_reopen_mic', True)
                        # Deepgram key update
                        dgk = self.cfg.get('deepgram_api_key')
                        if dgk and dgk != self.deepgram_key:
                            self.deepgram_key = dgk
                            try:
                                if self.asr_ws:
                                    self.asr_ws.close()
                            except Exception:
                                pass
                        # Debug flags
                        self.debug_agent = bool(self.cfg.get('debug_agent', False))
                        self.debug_tools = bool(self.cfg.get('debug_tools', False))
                        if self.debug_agent or self.debug_tools:
                            print(f"[cfg] Debug enabled: agent={self.debug_agent} tools={self.debug_tools}")
                        if not silent:
                            print(f"[cfg] Reloaded {self.config_path}")
                    self._cfg_mtime = st.st_mtime
        except Exception as e:
            if not silent:
                print(f"[cfg] Reload failed: {e}")

    def _ensure_playback_thread(self):
        try:
            if not (self.playback_thread and self.playback_thread.is_alive()):
                self.playback_thread = threading.Thread(target=self._audio_playback_worker, daemon=True)
                self.playback_thread.start()
                print("ðŸ”Š Audio playback thread started\n")
        except Exception:
            pass

    def _deepgram_functions_from_corrected_tools(self):
        """Convert OpenAI-style CORRECTED_TOOLS into Deepgram function schema.
        Expected CORRECTED_TOOLS item shape: {"type":"function","function":{name,description,parameters}}
        """
        funcs = []
        try:
            for t in CORRECTED_TOOLS:
                if not isinstance(t, dict):
                    continue
                fn = t.get("function") if "function" in t else None
                if not fn:
                    # fallback if already flat
                    fn = t
                name = (fn or {}).get("name")
                if not name:
                    continue
                funcs.append({
                    "name": name,
                    "description": fn.get("description", ""),
                    "parameters": fn.get("parameters", {"type": "object", "properties": {}}),
                })
        except Exception:
            pass
        return funcs

    def _config_watcher(self):
        while True:
            try:
                self._load_config(silent=True)
            except Exception:
                pass
            # Also hot-reload identity if it changes
            try:
                if self.identity_path.exists():
                    st = self.identity_path.stat()
                    if st.st_mtime != self._identity_mtime:
                        self.identity = self._load_identity()
                        self._identity_mtime = st.st_mtime
                        print(f"[identity] Reloaded {self.identity_path}")
            except Exception:
                pass
            time.sleep(2.0)

    def _prepare_tts_text(self, text: str) -> str:
        # When speak_symbols is False, remove punctuation/symbols entirely
        if not self.cfg.get('speak_symbols', False):
            t = text.replace('_', ' ').replace('-', ' ')
            t = re.sub(r"[^\w\s]", "", t, flags=re.UNICODE)
            t = re.sub(r"\s+", " ", t).strip()
            return t
        return text

    async def queue_audio_output(self, pcm_bytes: bytes):
        try:
            # Allow larger queue for prebuffered audio (non-blocking)
            if self.audio_queue.qsize() > 300:
                try:
                    _ = self.audio_queue.get_nowait()
                except queue.Empty:
                    pass
            self.audio_queue.put_nowait(pcm_bytes)
        except Exception as e:
            print(f"Audio queue error: {e}")

    async def _ask_server_respond(self, text: str) -> str:
        headers = { 'Content-Type': 'application/json' }
        # Try preferred route first
        route = str(self.cfg.get('server_route', 'respond')).lower()
        base = self.cfg.get('server_url', f"http://127.0.0.1:5051/{route}")
        def _pack(r: str):
            if r == 'respond':
                return base if base.endswith('/respond') else base.rsplit('/',1)[0] + '/respond', json.dumps({
                    "sessionId": "voice-default",
                    "messages": [ { "role": "user", "content": text } ],
                    "run_tools": True,
                    "allow_write": True,
                    "persona": "AVA",
                    "style": "first_person",
                    "context": {
                        "identity": self.identity,
                        "uptime": self._uptime_hms(),
                        "platform": platform.system()
                    }
                }).encode('utf-8')
            else:
                return base if base.endswith('/chat') else base.rsplit('/',1)[0] + '/chat', json.dumps({
                    "sessionId": "voice-default",
                    "text": text,
                    "run_tools": True,
                    "allow_write": True,
                    "persona": "AVA",
                    "style": "first_person",
                    "context": {
                        "identity": self.identity,
                        "uptime": self._uptime_hms(),
                        "platform": platform.system()
                    }
                }).encode('utf-8')
        # Preferred
        url, body = _pack(route)
        req = urllib.request.Request(url=url, data=body, headers=headers, method='POST')
        try:
            with urllib.request.urlopen(req, timeout=30) as resp:
                raw = resp.read()
                j = json.loads(raw.decode('utf-8', errors='ignore'))
                return (j.get('output_text') or (j.get('content') or [{}])[0].get('text') or '').strip()
        except urllib.error.HTTPError as he:
            # Fallback to alternate route on 5xx/4xx
            alt = 'chat' if route == 'respond' else 'respond'
            alt_url, alt_body = _pack(alt)
            try:
                req2 = urllib.request.Request(url=alt_url, data=alt_body, headers=headers, method='POST')
                with urllib.request.urlopen(req2, timeout=30) as resp2:
                    raw = resp2.read()
                    j = json.loads(raw.decode('utf-8', errors='ignore'))
                    return (j.get('output_text') or (j.get('content') or [{}])[0].get('text') or '').strip()
            except Exception as e2:
                print(f"[route] Server error fallback: {e2}")
                return ''
        except Exception as e:
            print(f"[route] Server error: {e}")
            return ''

    # ---------------------- Server supervision (when not using hot runner) ----------------------
    def _server_up(self, url: str, timeout: float = 2.0) -> bool:
        try:
            # Probe base URL via GET (robust against route 5xx)
            base = url.rsplit('/', 1)[0] if url.endswith('/respond') or url.endswith('/chat') else url
            req = urllib.request.Request(base, method='GET')
            with urllib.request.urlopen(req, timeout=timeout) as resp:
                code = getattr(resp, 'status', None) or getattr(resp, 'code', None) or 200
                return 200 <= code < 500
        except Exception:
            return False

    def _spawn_server(self) -> subprocess.Popen | None:
        # Start node server.js in ../ava-server if present
        try:
            base = Path(__file__).resolve().parent
            server_dir = base.parent / "ava-server"
            if not server_dir.exists():
                print(f"[server] Directory not found: {server_dir}")
                return None
            env = os.environ.copy()
            proc = subprocess.Popen(["node", "server.js"], cwd=str(server_dir), env=env)
            print(f"[server] Started server.js (PID {proc.pid})")
            return proc
        except Exception as e:
            print(f"[server] Failed to start: {e}")
            return None

    def _ensure_server_started(self):
        try:
            url = self.cfg.get('server_url', "http://127.0.0.1:5051/respond")
            if self._server_up(url):
                print(f"[server] Up: {url}")
                return
            print("[server] Down. Attempting to startâ€¦")
            sp = self._spawn_server()
            # Wait briefly for it to come up
            for _ in range(10):
                time.sleep(0.7)
                if self._server_up(url):
                    print(f"[server] Up after start: {url}")
                    return
            print("[server] Still down after start attempts.")
        except Exception:
            pass

    async def _speak_text(self, text: str):
        if not text:
            return
        speak_text = self._prepare_tts_text(text)
        if not speak_text:
            return
        self.tts_active.set()
        speak_url = f"{DG_SPEAK_BASE}&encoding=linear16&sample_rate={self.playback_rate}"
        req = urllib.request.Request(
            url=speak_url,
            data=json.dumps({"text": speak_text}).encode('utf-8'),
            headers={
                'Authorization': f'Token {self.deepgram_key}',
                'Content-Type': 'application/json',
                'Accept': 'audio/wav'
            },
            method='POST'
        )
        ctx = ssl.create_default_context()
        try:
            # Prebuffer entire audio for smooth playback
            audio_buffer = bytearray()
            with urllib.request.urlopen(req, context=ctx, timeout=60) as resp:
                # Skip WAV header
                hdr = resp.read(44)
                # Download entire audio first
                while True:
                    chunk = resp.read(32768)
                    if not chunk:
                        break
                    audio_buffer.extend(chunk)

            # Now play the buffered audio in large chunks
            chunk_size = 65536  # Larger chunks for smoother playback
            for i in range(0, len(audio_buffer), chunk_size):
                if self.user_speaking.is_set():
                    break
                chunk = bytes(audio_buffer[i:i+chunk_size])
                await self.queue_audio_output(chunk)
                # No sleep - let the playback thread handle timing

        except Exception as e:
            print(f"TTS error: {e}")
        finally:
            self.tts_active.clear()
            try:
                self.metrics['tts_utterances'] += 1
            except Exception:
                pass

    def _listen_url(self) -> str:
        model = self.cfg.get('asr_model') or 'nova-2'
        return (
            "wss://api.deepgram.com/v1/listen?encoding=linear16"
            f"&sample_rate={MIC_RATE}&channels=1&model={model}&smart_format=true"
        )

    async def connect_asr(self):
        print("\nðŸŽ¤ Connecting to Deepgram Live (ASR)...")
        headers = { 'Authorization': f'Token {self.deepgram_key}' }
        url = self._listen_url()
        try:
            self.asr_ws = await websockets.connect(
                url,
                extra_headers=headers,
                ping_interval=20,
                ping_timeout=20,
                max_queue=None,
                close_timeout=10,
            )
        except TypeError:
            self.asr_ws = await websockets.connect(
                url,
                additional_headers=headers,
                ping_interval=20,
                ping_timeout=20,
                max_queue=None,
                close_timeout=10,
            )
        print("âœ… ASR connected")

    async def close_asr(self):
        try:
            if self.asr_ws:
                await self.asr_ws.close()
        except:
            pass
        self.asr_ws = None

    async def asr_receiver(self):
        print("ðŸ‘‚ Listening for ASR events...\n")
        try:
            async for message in self.asr_ws:
                if isinstance(message, (bytes, bytearray)):
                    continue
                try:
                    event = json.loads(message)
                except Exception:
                    continue
                if self.cfg.get('debug_asr'):
                    try:
                        js = json.dumps(event)
                        preview = (js[:400] + '...') if len(js) > 400 else js
                        print(f"[ASR] {preview}")
                    except Exception:
                        pass
                alt = None
                is_final = False
                try:
                    # Deepgram ASR has is_final and channel at top level
                    is_final = bool(event.get('is_final', False))
                    ch = event.get('channel')
                    if ch and isinstance(ch, dict):
                        alts = ch.get('alternatives')
                        if alts and isinstance(alts, list) and len(alts) > 0:
                            alt = alts[0]
                except Exception:
                    pass
                transcript = ''
                if isinstance(alt, dict):
                    transcript = alt.get('transcript', '')
                try:
                    self.metrics['asr_messages'] += 1
                except Exception:
                    pass
                if transcript:
                    if is_final:
                        print(f"\nðŸ—£ï¸  You: {transcript}")
                        try:
                            self.metrics['asr_finals'] += 1
                        except Exception:
                            pass
                        
                        # Check if this is a correction of AVA's last response
                        if self._detect_correction(transcript):
                            self._handle_correction(transcript)
                        
                        # Intercept self-awareness queries locally
                        handled = await self._maybe_handle_local_intent(transcript)
                        if handled:
                            continue
                        
                        # Check for past mistakes and enhance transcript if needed
                        enhanced_transcript = self._get_enhanced_transcript(transcript)
                        
                        reply = await self._ask_server_respond(enhanced_transcript)
                        if reply:
                            print(f"ðŸ¤– AVA: {reply}")
                            await self._speak_text(reply)
                            
                            # Track for correction detection
                            self._last_user_transcript = transcript
                            self._last_ava_response = reply
                            
                            # Record interaction for passive learning
                            if self.passive_learning_enabled and PASSIVE_LEARNING_AVAILABLE:
                                try:
                                    record_interaction(transcript, reply, True)
                                except:
                                    pass
                    else:
                        # interim ignored
                        pass
        except websockets.exceptions.ConnectionClosed as e:
            print(f"\nðŸ”Œ ASR connection closed: {e}")
            try:
                self.metrics['reconnects'] += 1
            except Exception:
                pass
        except Exception as e:
            print(f"\nâŒ ASR receiver error: {e}")
            try:
                self.metrics['last_error'] = str(e)
            except Exception:
                pass

    async def stream_microphone_input(self):
        """Stream microphone input to Deepgram ASR"""
        def open_mic():
            kwargs = dict(format=FORMAT, channels=CHANNELS, rate=MIC_RATE, input=True, frames_per_buffer=CHUNK_SAMPLES)
            if self.input_device_index is not None:
                kwargs['input_device_index'] = self.input_device_index
            return self.audio.open(**kwargs)

        stream = open_mic()

        print("ðŸŽ¤ Microphone active - AVA is always listening!")

        try:
            while self.running:
                # Reopen mic on config changes
                if getattr(self, '_reopen_mic', False):
                    try:
                        stream.stop_stream(); stream.close()
                    except Exception:
                        pass
                    stream = open_mic()
                    setattr(self, '_reopen_mic', False)
                audio_data = stream.read(CHUNK_SAMPLES, exception_on_overflow=False)
                # VAD gating during active TTS
                rms = self._rms_int16(audio_data)
                if self.cfg.get('debug_rms'):
                    if int(time.time()*2) % 10 == 0:
                        print(f"[mic] rms={int(rms)}")
                now = time.time()
                if self.tts_active.is_set():
                    if not self.user_speaking.is_set():
                        if rms >= self.START_THRESH:
                            self._loud_frames += 1
                            if self._loud_frames >= 3:
                                self.user_speaking.set()
                                self._last_user_voice_t = now
                                self._cancel_tts()
                        else:
                            self._loud_frames = 0
                    else:
                        if rms >= self.STOP_THRESH:
                            self._last_user_voice_t = now
                        elif (now - self._last_user_voice_t) > self.SPEECH_HOLD_SEC:
                            self.user_speaking.clear()
                    if not self.user_speaking.is_set():
                        await asyncio.sleep(CHUNK_SAMPLES / MIC_RATE)
                        continue

                if self.asr_ws is not None:
                    try:
                        await self.asr_ws.send(audio_data)
                    except Exception:
                        await asyncio.sleep(0.02)
                await asyncio.sleep(CHUNK_SAMPLES / MIC_RATE)

        finally:
            stream.stop_stream()
            stream.close()

    def _audio_playback_worker(self):
        """Worker thread for continuous audio playback"""
        # Open persistent playback stream (with dynamic re-open support)
        def open_playback():
            try:
                open_kwargs = dict(format=FORMAT, channels=CHANNELS, rate=self.playback_rate, output=True, frames_per_buffer=4096)
                if self.output_device_index is not None:
                    open_kwargs['output_device_index'] = self.output_device_index
                # Log selected output device
                try:
                    if 'output_device_index' in open_kwargs:
                        info = self.audio.get_device_info_by_index(open_kwargs['output_device_index'])
                    else:
                        info = self.audio.get_default_output_device_info()
                    print(f"[audio] Using output device: {info.get('name')} (idx={info.get('index')}) @ {self.playback_rate} Hz")
                except Exception:
                    pass
                return self.audio.open(**open_kwargs)
            except Exception:
                try:
                    self.playback_rate = 48000
                    open_kwargs = dict(format=FORMAT, channels=CHANNELS, rate=self.playback_rate, output=True, frames_per_buffer=4096)
                    if self.output_device_index is not None:
                        open_kwargs['output_device_index'] = self.output_device_index
                    print(f"[audio] Fallback playback rate {self.playback_rate} Hz")
                    try:
                        if 'output_device_index' in open_kwargs:
                            info = self.audio.get_device_info_by_index(open_kwargs['output_device_index'])
                        else:
                            info = self.audio.get_default_output_device_info()
                        print(f"[audio] Using output device: {info.get('name')} (idx={info.get('index')}) @ {self.playback_rate} Hz")
                    except Exception:
                        pass
                    return self.audio.open(**open_kwargs)
                except Exception as e:
                    print(f"Playback open error: {e}")
                    # Auto-select a viable output device
                    try:
                        dev_count = self.audio.get_device_count()
                        for idx in range(dev_count):
                            try:
                                info = self.audio.get_device_info_by_index(idx)
                                if int(info.get('maxOutputChannels', 0)) <= 0:
                                    continue
                                test_kwargs = dict(format=FORMAT, channels=CHANNELS, rate=self.playback_rate, output=True, frames_per_buffer=2048, output_device_index=idx)
                                stream = self.audio.open(**test_kwargs)
                                print(f"[audio] Auto-selected output: {info.get('name')} (idx={idx}) @ {self.playback_rate} Hz")
                                return stream
                            except Exception:
                                continue
                        print("[audio] No suitable output device found.")
                    except Exception:
                        pass
                    return None

        self.playback_stream = open_playback()
        if self.playback_stream is None:
            return

        try:
            while self.running:
                try:
                    # Reopen playback if requested by config changes
                    if getattr(self, '_reopen_playback', False):
                        try:
                            self.playback_stream.stop_stream()
                            self.playback_stream.close()
                        except Exception:
                            pass
                        self.playback_stream = open_playback()
                        setattr(self, '_reopen_playback', False)
                        if self.playback_stream is None:
                            time.sleep(0.5)
                            continue
                    # Get audio chunk from queue with timeout
                    audio_data = self.audio_queue.get(timeout=0.1)

                    if audio_data is None:  # Poison pill to stop thread
                        break

                    # Write audio chunk to stream
                    try:
                        self.playback_busy.set()
                        self.playback_stream.write(audio_data)
                    finally:
                        self.playback_busy.clear()
                    # Update global playback RMS EMA for echo gating
                    try:
                        pr = self._rms_int16(audio_data)
                        self._playback_rms_ema = (self._playback_rms_ema * 0.85) + (pr * 0.15)
                    except Exception:
                        pass

                except queue.Empty:
                    continue
                except Exception as e:
                    print(f"Playback error: {e}")

        finally:
            if self.playback_stream:
                self.playback_stream.stop_stream()
                self.playback_stream.close()

    # ---------------------- Voice Engine Switching ----------------------

    def _switch_to_local_voice(self):
        """Switch from Deepgram to local Whisper + Edge TTS"""
        with self.voice_engine_state.lock:
            if self.voice_engine_state.current == VoiceEngineState.LOCAL:
                return  # Already on local
            if not self.local_voice_engine:
                print("[voice] Cannot switch to local - engine not available")
                return
            
            self.voice_engine_state.current = VoiceEngineState.SWITCHING
            print("[voice] âš¡ Switching to LOCAL voice engine (Whisper + Edge TTS)...")
        
        # Update state
        with self.voice_engine_state.lock:
            self.voice_engine_state.current = VoiceEngineState.LOCAL
            self.voice_engine_state.deepgram_available = False
            self.voice_engine_state.last_deepgram_check = time.time()
        
        # Start local voice engine in a thread
        threading.Thread(target=self.local_voice_engine.run, name="local_voice", daemon=True).start()
        
        print("[voice] âœ… Now using LOCAL voice engine")

    def _switch_to_deepgram(self):
        """Switch from local back to Deepgram"""
        with self.voice_engine_state.lock:
            if self.voice_engine_state.current == VoiceEngineState.DEEPGRAM:
                return  # Already on Deepgram
            
            self.voice_engine_state.current = VoiceEngineState.SWITCHING
            print("[voice] âš¡ Switching to DEEPGRAM voice engine...")
        
        # Stop local engine
        if self.local_voice_engine:
            self.local_voice_engine.stop()
        
        # Wait for clean handoff
        time.sleep(0.5)
        
        with self.voice_engine_state.lock:
            self.voice_engine_state.current = VoiceEngineState.DEEPGRAM
            self.voice_engine_state.deepgram_available = True
            self.voice_engine_state.consecutive_errors = 0
        
        print("[voice] âœ… Now using DEEPGRAM voice engine")

    def _check_deepgram_available(self) -> bool:
        """Check if Deepgram API is available (not quota exhausted)"""
        try:
            import urllib.request
            import urllib.error
            
            # Simple API check - get project info
            url = "https://api.deepgram.com/v1/projects"
            req = urllib.request.Request(url)
            req.add_header("Authorization", f"Token {self.deepgram_key}")
            req.add_header("Content-Type", "application/json")
            
            ctx = ssl.create_default_context()
            with urllib.request.urlopen(req, timeout=10, context=ctx) as response:
                if response.status == 200:
                    print("[deepgram] API check passed - quota available")
                    return True
                    
        except urllib.error.HTTPError as e:
            if e.code == 402:
                print(f"[deepgram] âš ï¸ Quota exhausted (HTTP 402)")
                return False
            elif e.code == 401:
                print(f"[deepgram] âš ï¸ Invalid API key (HTTP 401)")
                return False
            else:
                print(f"[deepgram] HTTP error {e.code}: {e.reason}")
                # Other errors might be transient, assume available
                return True
        except Exception as e:
            print(f"[deepgram] Check failed: {e}")
            # Network error, assume available and let it fail later
            return True
        
        return True

    def _handle_deepgram_quota_error(self):
        """Handle Deepgram quota exceeded - switch to local if available"""
        self.voice_engine_state.consecutive_errors += 1
        errors = self.voice_engine_state.consecutive_errors
        threshold = self.voice_engine_state.error_threshold
        
        print(f"[voice] Deepgram quota error ({errors}/{threshold})")
        
        if errors >= threshold and self.local_voice_engine:
            self._switch_to_local_voice()
            return True  # Signal to stop Deepgram loop
        
        return False

    # ---------------------- Deepgram Agent Voice (proven path) ----------------------

    def run_agent_voice(self):
        client = DeepgramClient(api_key=self.deepgram_key)
        p = pyaudio.PyAudio()
        out_kwargs = dict(format=pyaudio.paInt16, channels=1, rate=24000, output=True)
        if self.output_device_index is not None:
            out_kwargs['output_device_index'] = self.output_device_index
        # Open speaker with fallbacks and logging
        try:
            try:
                if 'output_device_index' in out_kwargs:
                    info = p.get_device_info_by_index(out_kwargs['output_device_index'])
                else:
                    info = p.get_default_output_device_info()
                print(f"[audio] Agent using output device: {info.get('name')} (idx={info.get('index')}) @ {out_kwargs['rate']} Hz")
            except Exception:
                pass
            speaker_stream = p.open(**out_kwargs)
        except Exception:
            try:
                out_kwargs['rate'] = 48000
                if 'output_device_index' in out_kwargs:
                    info = p.get_device_info_by_index(out_kwargs['output_device_index'])
                else:
                    info = p.get_default_output_device_info()
                print(f"[audio] Agent fallback device: {info.get('name')} (idx={info.get('index')}) @ {out_kwargs['rate']} Hz")
                speaker_stream = p.open(**out_kwargs)
            except Exception as e:
                print(f"Agent speaker open error: {e}")
                # Try auto-selecting any viable output device
                speaker_stream = None
                try:
                    dev_count = p.get_device_count()
                    for idx in range(dev_count):
                        try:
                            info = p.get_device_info_by_index(idx)
                            if int(info.get('maxOutputChannels', 0)) <= 0:
                                continue
                            test_kwargs = dict(format=pyaudio.paInt16, channels=1, rate=48000, output=True, frames_per_buffer=1920, output_device_index=idx)
                            speaker_stream = p.open(**test_kwargs)
                            print(f"[audio] Agent auto-selected output: {info.get('name')} (idx={idx}) @ 48000 Hz")
                            break
                        except Exception:
                            continue
                except Exception:
                    pass
                if speaker_stream is None:
                    raise RuntimeError("No suitable output device for agent speech")
        in_kwargs = dict(format=FORMAT, channels=CHANNELS, rate=MIC_RATE, input=True, frames_per_buffer=1024)
        if self.input_device_index is not None:
            in_kwargs['input_device_index'] = self.input_device_index
        mic_stream = p.open(**in_kwargs)

        shutdown = threading.Event()
        connection_active = threading.Event()
        conn_ref = {"conn": None}
        wav_stripper = WavToPcmStripper()
        agent_tts_fallback = threading.Event()
        non_audio_ctr = {"n": 0}  # Counter for debugging non-audio messages

        # Agent audio playback queue for non-blocking writes
        agent_audio_queue = queue.Queue()
        agent_playback_active = threading.Event()

        # Shared state for echo/interrupt control
        playback_rms = {"v": 0.0}
        tts_start_time = {"t": 0.0}
        last_tts_pcm_time = {"t": 0.0}
        barge_cfg = self.cfg.get('barge') or {}
        debounce_frames = int(barge_cfg.get('debounce_frames', 3))
        dyn_scale = float(barge_cfg.get('dyn_thresh_scale', 0.6))
        min_tts_ms = int(barge_cfg.get('min_tts_ms', 300))
        # Stricter barge-in while TTS is playing to avoid echo trigger from speakers
        strict_dyn_scale = float(barge_cfg.get('strict_dyn_scale', 2.2))
        strict_debounce_frames = int(barge_cfg.get('strict_debounce_frames', max(6, debounce_frames + 4)))

        # Audio send counter for debugging
        audio_send_ctr = {"n": 0}

        # Connection health tracking for watchdog
        last_rx_time = {"t": time.time()}
        last_tx_time = {"t": time.time()}

        # Local barge mode flag for agent voice
        barge_mode = threading.Event()

        # Watchdog to clear stuck TTS state if AgentAudioDone is missed
        def tts_watchdog():
            while not shutdown.is_set():
                try:
                    if self.tts_active.is_set():
                        now = time.time()
                        base = max(tts_start_time.get("t", 0.0), last_tts_pcm_time.get("t", 0.0))
                        if base and (now - base) > 3.0:
                            self.tts_active.clear()
                            tts_start_time["t"] = 0.0
                            last_tts_pcm_time["t"] = 0.0
                            playback_rms["v"] = 0.0
                            self.user_speaking.clear()
                            barge_mode.clear()
                            wav_stripper.reset()
                            agent_tts_fallback.clear()
                            # Clear playback queue
                            try:
                                while not agent_audio_queue.empty():
                                    agent_audio_queue.get_nowait()
                            except Exception:
                                pass
                except Exception:
                    pass
                time.sleep(0.5)

        def connection_watchdog():
            """Force reconnect if idle >25s (no RX or TX activity)"""
            IDLE_TIMEOUT = 25.0
            while not shutdown.is_set():
                time.sleep(5.0)
                if connection_active.is_set():
                    now = time.time()
                    idle_for = now - max(last_rx_time["t"], last_tx_time["t"])
                    if idle_for > IDLE_TIMEOUT:
                        print(f"[watchdog] Idle {idle_for:.1f}s, forcing reconnect")
                        try:
                            conn = conn_ref["conn"]
                            if conn is not None:
                                conn.close()
                        except Exception as ex:
                            print(f"[watchdog] Close error: {ex}")
                        connection_active.clear()

        def agent_playback_thread():
            """Non-blocking playback thread for agent TTS audio"""
            agent_playback_active.set()

            try:
                while not shutdown.is_set():
                    try:
                        # Get audio from queue with timeout
                        pcm_data = agent_audio_queue.get(timeout=0.05)

                        if pcm_data is None:  # Poison pill to stop thread
                            break

                        # Handle barge-in: drop audio if user is speaking
                        if barge_mode.is_set():
                            print(f"[PLAYBACK] Dropping audio chunk (barge_mode active)")
                            continue

                        # Write to speaker stream (non-blocking for message handler)
                        try:
                            speaker_stream.write(pcm_data)
                        except Exception as e:
                            # If write fails, try to continue with next chunk
                            pass
                    except queue.Empty:
                        # Just wait for more audio - don't clear tts_active here
                        # The message handler will clear it when AgentAudioDone arrives
                        continue
                    except Exception:
                        pass
            finally:
                agent_playback_active.clear()

        def microphone_thread():
            nonlocal mic_stream
            loud_frames = 0
            while not shutdown.is_set():
                try:
                    # Hot-reopen mic if config changed
                    if getattr(self, '_reopen_mic', False):
                        try:
                            mic_stream.stop_stream(); mic_stream.close()
                        except Exception:
                            pass
                        try:
                            kwargs = dict(format=FORMAT, channels=CHANNELS, rate=MIC_RATE, input=True, frames_per_buffer=1024)
                            if self.input_device_index is not None:
                                kwargs['input_device_index'] = self.input_device_index
                            mic_stream = p.open(**kwargs)
                            try:
                                info_in = p.get_device_info_by_index(kwargs['input_device_index']) if 'input_device_index' in kwargs else p.get_default_input_device_info()
                                print(f"[audio] Reopened mic: {info_in.get('name')} (idx={info_in.get('index')}) @ {MIC_RATE} Hz")
                            except Exception:
                                pass
                        except Exception as _e:
                            print(f"[audio] Mic reopen failed: {_e}")
                        setattr(self, '_reopen_mic', False)
                except Exception:
                    pass
                try:
                    data = mic_stream.read(480, exception_on_overflow=False)
                except Exception:
                    time.sleep(0.01)
                    continue
                # Half-duplex echo control with dynamic threshold and debounce
                rms = self._rms_int16(data)
                # Optional RMS debug output to verify mic capture
                try:
                    if bool(self.cfg.get('debug_rms', False)):
                        if int(time.time()*2) % 10 == 0:
                            print(f"[mic] rms={int(rms)}")
                except Exception:
                    pass
                now = time.time()
                # Echo-aware barge-in policy during playback (matching avas_voice.py pattern)
                if self.tts_active.is_set():
                    if not self.user_speaking.is_set():
                        # Dynamic threshold based on far-end playback level
                        prms = max(self._playback_rms_ema, playback_rms["v"])
                        dyn_thresh = max(self.START_THRESH, prms * dyn_scale)  # Use config variable
                        if rms >= dyn_thresh:
                            loud_frames += 1
                            if loud_frames >= debounce_frames:
                                self.user_speaking.set()
                                barge_mode.set()  # Drop TTS while user speaks
                                self._last_user_voice_t = now
                        else:
                            loud_frames = 0
                    else:
                        # Maintain speaking state with hysteresis and hold
                        if rms >= self.STOP_THRESH:
                            self._last_user_voice_t = now
                        elif (now - self._last_user_voice_t) > self.SPEECH_HOLD_SEC:
                            self.user_speaking.clear()
                            barge_mode.clear()
                            loud_frames = 0
                    # If TTS is active and user isn't speaking, drop mic frames (like avas_voice.py)
                    if not self.user_speaking.is_set():
                        # Send periodic keepalive silence to prevent timeout
                        if (now - last_tx_time.get("t", 0)) > 5.0:
                            silent = b"\x00" * 480 * 2  # 480 samples at 16kHz, 16-bit mono
                            if connection_active.is_set():
                                try:
                                    c = conn_ref["conn"]
                                    if c is not None:
                                        c.send_media(silent)
                                        last_tx_time["t"] = now
                                except Exception:
                                    pass
                        continue  # Drop this frame, don't send it
                else:
                    # No TTS; reset state
                    barge_mode.clear()
                    self.user_speaking.clear()
                    loud_frames = 0

                # Send real audio upstream
                if connection_active.is_set():
                    try:
                        c = conn_ref["conn"]
                        if c is not None:
                            c.send_media(data)
                            last_tx_time["t"] = time.time()
                            audio_send_ctr["n"] += 1
                    except Exception as e:
                        pass
                else:
                    time.sleep(0.01)

        threading.Thread(target=tts_watchdog, name="tts_watch", daemon=True).start()
        threading.Thread(target=connection_watchdog, name="conn_watchdog", daemon=True).start()
        threading.Thread(target=agent_playback_thread, name="agent_playback", daemon=True).start()
        threading.Thread(target=microphone_thread, name="agent_mic", daemon=True).start()

        # Pre-build settings message ONCE before connection loop to minimize delay
        ident = self.identity
        name = ident.get('name', 'AVA')
        dev = ident.get('developer', 'your developer')
        purpose = ident.get('purpose', 'your assistant on this laptop')
        
        # Get personality context if available
        personality_context = ""
        if PERSONALITY_AVAILABLE:
            try:
                personality_context = get_personality_context()
            except Exception as e:
                print(f"[personality] Error loading context: {e}")
        
        # Get self-awareness learning context if available
        learning_context = ""
        if SELF_AWARENESS_AVAILABLE and self.self_awareness_enabled:
            try:
                learning_context = get_prompt_context()
            except Exception as e:
                print(f"[self-awareness] Error loading context: {e}")
        
        prompt_text = (
            f"You are {name}, my on-device assistant built by {dev}. "
            f"You run locally on this laptop and operate as the AVA agent. "
            f"Purpose: {purpose}. "
        )
        
        # Add personality context if available
        if personality_context:
            prompt_text += f" {personality_context} "
        
        # Add self-awareness learning context if available
        if learning_context:
            prompt_text += f" LEARNED CONTEXT: {learning_context} "
        
        # Add passive context (current screen/environment awareness)
        if PASSIVE_LEARNING_AVAILABLE and self.passive_learning_enabled:
            try:
                passive_ctx = get_passive_context()
                if passive_ctx.get("active_app") and passive_ctx["active_app"] != "unknown":
                    prompt_text += f" CURRENT CONTEXT: User is in {passive_ctx['active_app']} ({passive_ctx.get('context_type', 'general')} activity). "
            except:
                pass
        
        prompt_text += (
            f"Behavioral contract: You are the voice for the AVA agent runtime. "
            f"Self-awareness: You can describe your identity, uptime, platform, and install location. "
            f"Tool calling: NEVER output JSON tool calls in your assistant text. "
            f"Use the function calling interface provided by the system instead. "
            f"When you need to call a tool, use the native function calling mechanism - do NOT print JSON. "
            f"If no tool is needed, respond concisely in first person. "
            f"Never claim an action is complete unless the tool result confirms it. "
            f"Speech policy: Do not read punctuation or decorative symbols aloud. "
            f"Treat characters like *, #, _, ~, backticks, code fences, and emoji as silent unless explicitly asked to read them. "
            f"If text includes markup or formatting symbols, convey the meaning in natural speech instead of pronouncing symbols."
        )
        dg_functions = self._deepgram_functions_from_corrected_tools()

        # Get available think providers for fallback
        available_providers = self._get_available_think_providers()
        if not available_providers:
            print("[agent] ERROR: No LLM providers available for think!")
            return

        current_provider_idx = {"idx": 0}  # Track which provider we're using

        def build_settings_with_provider(provider_name, provider_class, model_name):
            """Helper to build settings with a specific think provider"""
            # Determine provider type string based on class
            if provider_class == AgentV1GoogleThinkProvider:
                provider_type = "google"
            elif provider_class == AgentV1AnthropicThinkProvider:
                provider_type = "anthropic"
            elif provider_class == AgentV1OpenAiThinkProvider:
                provider_type = "open_ai"
            elif provider_class == AgentV1GroqThinkProvider:
                provider_type = "groq"
            else:
                provider_type = "unknown"

            # Deepgram manages all providers natively - no custom endpoints needed
            # This avoids INVALID_SETTINGS errors from endpoint conflicts
            think_config = AgentV1Think(
                provider=provider_class(type=provider_type, model=model_name),
                prompt=prompt_text,
                functions=dg_functions
            )

            return AgentV1SettingsMessage(
                audio=AgentV1AudioConfig(
                    input=AgentV1AudioInput(encoding="linear16", sample_rate=MIC_RATE),
                    output=AgentV1AudioOutput(encoding="linear16", sample_rate=24000, container="wav")
                ),
                agent=AgentV1Agent(
                    language="en",
                    listen=AgentV1Listen(
                        provider=AgentV1ListenProvider(type="deepgram", model="nova-2")
                    ),
                    think=think_config,
                    speak=AgentV1SpeakProviderConfig(
                        provider=AgentV1DeepgramSpeakProvider(type="deepgram", model="aura-2-andromeda-en")
                    )
                )
            )

        # Start with the first provider
        provider_name, provider_class, model_name = available_providers[current_provider_idx["idx"]]
        settings_obj = build_settings_with_provider(provider_name, provider_class, model_name)
        print(f"[agent] Settings object built: ASR={self.cfg.get('asr_model','nova-2')}, TTS={self.cfg.get('tts_model','aura-2-andromeda-en')}, Think={provider_name} ({model_name})")

        try:
            while not shutdown.is_set():
                try:
                    with client.agent.v1.connect() as connection:
                        conn_ref["conn"] = connection  # Store connection immediately

                        suppress_agent_tts = bool(self.cfg.get('suppress_agent_tts', True))

                        def on_message(message: AgentV1SocketClientResponse):
                            if isinstance(message, bytes):
                                # Strip WAV header and get PCM data (like avas_voice.py)
                                pcm = wav_stripper.feed(message)
                                if pcm:
                                    self.tts_active.set()
                                    if tts_start_time["t"] == 0.0:
                                        tts_start_time["t"] = time.time()
                                    # Update far-end playback RMS (EMA) for echo detection
                                    try:
                                        frame_rms = self._rms_int16(pcm)
                                        playback_rms["v"] = (playback_rms["v"] * 0.85) + (frame_rms * 0.15)
                                        self._playback_rms_ema = (self._playback_rms_ema * 0.85) + (frame_rms * 0.15)
                                    except Exception:
                                        pass
                                    last_tts_pcm_time["t"] = time.time()
                                    last_rx_time["t"] = time.time()
                                    # If barge mode is active (user speaking), drop playback to prevent echo
                                    if barge_mode.is_set():
                                        return
                                    # DIRECT WRITE to speaker (like avas_voice.py line 286)
                                    try:
                                        speaker_stream.write(pcm)
                                    except Exception:
                                        pass
                            else:
                                msg_type = getattr(message, "type", "Unknown")
                                try:
                                    # Always log messages when debug enabled
                                    if self.cfg.get('debug_asr') or non_audio_ctr["n"] < 12:
                                        payload = getattr(message, "__dict__", None)
                                        s = str(payload) if payload is not None else str(message)
                                        print(f"NON-AUDIO MSG: {msg_type} :: {s[:500]}")
                                        non_audio_ctr["n"] += 1
                                except Exception:
                                    pass

                                # Log UserStartedSpeaking with tts_active state for debugging
                                if msg_type == "UserStartedSpeaking":
                                    print(f"[TTS-CONTROL] UserStartedSpeaking (tts_active={self.tts_active.is_set()})")

                                # Handle errors with provider fallback
                                if msg_type == "Error":
                                    error_code = getattr(message, "code", None)
                                    error_desc = getattr(message, "description", "")
                                    print(f"[agent] ERROR from Deepgram: {error_code} - {error_desc}")

                                    # Check for errors that should trigger provider fallback
                                    should_fallback = False
                                    if error_code in ["INVALID_SETTINGS", "MODEL_ERROR", "QUOTA_EXCEEDED"]:
                                        should_fallback = True
                                    elif "quota" in error_desc.lower() or "limit" in error_desc.lower() or "rate limit" in error_desc.lower():
                                        should_fallback = True

                                    if should_fallback and current_provider_idx["idx"] < len(available_providers) - 1:
                                        # Try next provider
                                        current_provider_idx["idx"] += 1
                                        provider_name, provider_class, model_name = available_providers[current_provider_idx["idx"]]
                                        print(f"[agent] Falling back to {provider_name} ({model_name})...")
                                        settings_obj = build_settings_with_provider(provider_name, provider_class, model_name)
                                        # Close current connection to trigger reconnect with new provider
                                        try:
                                            conn.close()
                                        except Exception:
                                            pass
                                        connection_active.clear()
                                        return
                                    elif should_fallback:
                                        print(f"[agent] No more LLM providers to fall back to.")
                                        # Try switching to local voice engine (Whisper + Edge TTS)
                                        if self._handle_deepgram_quota_error():
                                            connection_active.clear()
                                            return  # Exit to let local engine take over

                                if msg_type == "ConversationText":
                                    last_rx_time["t"] = time.time()  # Track RX for connection watchdog
                                    try:
                                        role = str(getattr(message, 'role', ''))
                                        content = str(getattr(message, 'content', ''))
                                        # Show assistant text as AVA to match heard voice; tools will override with natural result
                                        if role == 'assistant':
                                            print(f"AVA: {content}")
                                        else:
                                            print(f"You: {content}")
                                        # Self-awareness and tools routing: on user text, handle local intents first,
                                        # then try corrected tools via cmpuse Agent; else call AVA server and speak reply
                                        if role == 'user' and content.strip():
                                            try:
                                                import asyncio
                                                loop = None
                                                try:
                                                    loop = asyncio.get_event_loop()
                                                except Exception:
                                                    loop = None
                                                # Handle self-awareness/intents locally first
                                                handled = False
                                                if loop and loop.is_running():
                                                    fut0 = asyncio.run_coroutine_threadsafe(self._maybe_handle_local_intent(content), loop)
                                                    handled = bool(fut0.result(timeout=10))
                                                else:
                                                    handled = asyncio.run(self._maybe_handle_local_intent(content))
                                                if handled:
                                                    return
                                                # DISABLED: Local tool handling - let Deepgram handle function calling
                                                # Do NOT speak tool results locally - only Deepgram speaks
                                                # This allows native FunctionCallRequest flow
                                            except Exception:
                                                # On server error, fall back to agent TTS
                                                agent_tts_fallback.set()
                                                pass
                                        # When suppressing agent TTS, skip speaking agent assistant text here.
                                    except Exception:
                                        pass
                                elif msg_type == "AgentText":
                                    # Do not parse or execute tools from AgentText; native FunctionCallRequest is the source of truth
                                    try:
                                        txt = str(getattr(message, 'content', ''))
                                        # Show what the Agent says as AVA for consistency
                                        if txt:
                                            print(f"AVA: {txt}")
                                    except Exception:
                                        pass
                                elif msg_type == "FunctionCallRequest":
                                    # Handle native Deepgram Voice Agent function calls per V1 spec
                                    print(f"[agent] Received FunctionCallRequest!")
                                    try:
                                        funcs_req = getattr(message, 'functions', None)
                                        if not funcs_req and isinstance(message, dict):
                                            funcs_req = message.get('functions')
                                        if not funcs_req:
                                            print("[agent] No functions in FunctionCallRequest")
                                            return

                                        conn = conn_ref.get('conn')
                                        print(f"[agent] Processing {len(funcs_req)} function call(s)")

                                        for f in funcs_req:
                                            try:
                                                # Check client_side flag per Deepgram V1 spec
                                                client_side = getattr(f, 'client_side', None) if hasattr(f, 'client_side') else (f.get('client_side') if isinstance(f, dict) else None)
                                                if client_side is not True:
                                                    print(f"[agent] Skipping function (client_side={client_side})")
                                                    continue

                                                call_id = getattr(f, 'id', None) if hasattr(f, 'id') else (f.get('id') if isinstance(f, dict) else None)
                                                tname = getattr(f, 'name', None) if hasattr(f, 'name') else (f.get('name') if isinstance(f, dict) else None)
                                                arg_str = getattr(f, 'arguments', '{}') if hasattr(f, 'arguments') else (f.get('arguments') if isinstance(f, dict) else '{}')

                                                print(f"[agent] Executing tool: {tname} (id={call_id})")

                                                try:
                                                    targs = json.loads(arg_str) if isinstance(arg_str, str) else (arg_str or {})
                                                except Exception:
                                                    targs = {}

                                                # Execute tool via CMPUSE
                                                res = asyncio.run(self.handle_tool_call(tname, targs))
                                                print(f"[agent] Tool result: {str(res)[:200]}")

                                                # Send FunctionCallResponse with same id and name per V1 spec
                                                payload = {
                                                    "type": "FunctionCallResponse",
                                                    "id": call_id,
                                                    "name": tname,
                                                    "content": json.dumps(res, default=str)
                                                }
                                                try:
                                                    conn.send(json.dumps(payload))
                                                    print(f"[agent] Sent FunctionCallResponse for {tname}")
                                                except Exception as send_ex:
                                                    print(f"[agent] Failed to send response: {send_ex}")
                                            except Exception as func_ex:
                                                print(f"[agent] Error processing function: {func_ex}")
                                        return
                                    except Exception as ex:
                                        print(f"[agent] FunctionCallRequest error: {ex}")
                                elif msg_type == "AgentAudioDone":
                                    print(f"[TTS-CONTROL] AgentAudioDone received, queue size: {agent_audio_queue.qsize()}")
                                    # Current TTS clip finished from Deepgram
                                    # But audio might still be in playback queue
                                    # Start a thread to wait for queue to drain, then clear tts_active
                                    def wait_for_queue_drain():
                                        # Wait for queue to empty
                                        while not agent_audio_queue.empty():
                                            time.sleep(0.05)
                                        # Wait extra 200ms for speaker buffer to drain
                                        time.sleep(0.2)
                                        print(f"[TTS-CONTROL] tts_active CLEAR (queue drained)")
                                        self.tts_active.clear()

                                    threading.Thread(target=wait_for_queue_drain, daemon=True).start()

                                    tts_start_time["t"] = 0.0
                                    playback_rms["v"] = 0.0
                                    self._playback_rms_ema = 0.0
                                    self.user_speaking.clear()
                                    if barge_mode.is_set():
                                        print(f"[BARGE] barge_mode CLEAR (AgentAudioDone)")
                                    barge_mode.clear()
                                    wav_stripper.reset()
                                    agent_tts_fallback.clear()

                        def on_close(close):
                            connection_active.clear(); conn_ref["conn"] = None

                        connection.on(EventType.MESSAGE, on_message)
                        connection.on(EventType.CLOSE, on_close)

                        # Send settings FIRST using proper SDK method (before start_listening)
                        try:
                            # DEBUG: Print exact JSON being sent to verify functions are included
                            try:
                                import json
                                settings_dict = settings_obj.__dict__ if hasattr(settings_obj, '__dict__') else settings_obj
                                # Try to serialize for debugging
                                def obj_to_dict(obj):
                                    if hasattr(obj, '__dict__'):
                                        result = {}
                                        for k, v in obj.__dict__.items():
                                            if isinstance(v, list):
                                                result[k] = [obj_to_dict(item) for item in v]
                                            elif hasattr(v, '__dict__'):
                                                result[k] = obj_to_dict(v)
                                            else:
                                                result[k] = v
                                        return result
                                    return obj
                                settings_json = obj_to_dict(settings_obj)
                                print(f"[agent] SENDING SETTINGS JSON:")
                                print(json.dumps(settings_json, indent=2))
                            except Exception as debug_ex:
                                print(f"[agent] Could not serialize settings for debug: {debug_ex}")

                            connection.send_settings(settings_obj)
                            print("[agent] Settings sent via SDK send_settings()")
                        except Exception as e:
                            print(f"[agent] Failed to send settings: {e}")
                            import traceback
                            traceback.print_exc()

                        # Activate the connection so mic can start sending audio
                        print("[agent] Setting connection_active flag...")
                        connection_active.set()
                        print(f"[agent] connection_active is now: {connection_active.is_set()}")

                        # NOW start listening for responses (this blocks to keep connection alive)
                        print("[agent] Calling start_listening()...")
                        connection.start_listening()
                        print("[agent] Deepgram Agent connected and configured")
                        while connection_active.is_set() and not shutdown.is_set():
                            time.sleep(0.1)
                except Exception as e:
                    error_str = str(e)
                    # Detect quota/payment issues (HTTP 402)
                    if "402" in error_str or "Payment Required" in error_str:
                        print(f"[agent] âš ï¸ Deepgram quota exhausted (HTTP 402)")
                        if LOCAL_FALLBACK_AVAILABLE and self.local_voice_engine:
                            print("[agent] Switching to local voice engine (Whisper + Edge TTS)...")
                            self._switch_to_local_voice()
                            # Keep running in local mode
                            while self.running and not shutdown.is_set():
                                time.sleep(0.5)
                            break  # Exit the reconnect loop
                        else:
                            print("[agent] Local fallback not available. Please add Deepgram credits.")
                            print("[agent] Visit: https://console.deepgram.com/billing")
                            break  # Don't keep retrying
                    else:
                        print(f"Agent connection error: {e}. Reconnecting in 3sâ€¦")
                    connection_active.clear(); time.sleep(3)
        finally:
            try:
                shutdown.set()
                mic_stream.stop_stream(); mic_stream.close()
                speaker_stream.stop_stream(); speaker_stream.close()
                p.terminate()
            except Exception:
                pass

    async def start_conversation(self):
        """Start the realtime voice conversation"""
        self.running = True

        # Ensure brain server is available if configured to auto-start
        if bool(self.cfg.get('auto_start_server', True)):
            self._ensure_server_started()

        # CHECK DEEPGRAM AVAILABILITY BEFORE STARTING
        # If quota is exhausted, start directly in local mode
        deepgram_available = self._check_deepgram_available()
        if not deepgram_available:
            if LOCAL_FALLBACK_AVAILABLE and self.local_voice_engine:
                print("[startup] âš¡ Deepgram unavailable - starting in LOCAL mode (Whisper + Edge TTS)")
                with self.voice_engine_state.lock:
                    self.voice_engine_state.current = VoiceEngineState.LOCAL
                    self.voice_engine_state.deepgram_available = False
                
                # Start local voice engine
                threading.Thread(target=self.local_voice_engine.run, name="local_voice", daemon=True).start()
                print("[startup] âœ… Local voice engine started")
                
                # Keep running in local mode
                try:
                    while self.running:
                        await asyncio.sleep(0.5)
                except KeyboardInterrupt:
                    print("\n\nðŸ‘‹ Shutting down AVA...")
                finally:
                    self.running = False
                    if self.local_voice_engine:
                        self.local_voice_engine.stop()
                return
            else:
                print("[startup] âš ï¸ Deepgram unavailable and no local fallback!")
                print("[startup] Please add Deepgram credits: https://console.deepgram.com/billing")
                print("[startup] Or install local fallback: pip install faster-whisper edge-tts")
                return

        # Ensure audio playback thread only when needed (local TTS mode)
        try:
            vmode = str(self.cfg.get('voice_mode', 'agent')).lower()
            use_local_tts = bool(self.cfg.get('suppress_agent_tts', True))
        except Exception:
            vmode = 'agent'; use_local_tts = True
        if (vmode != 'agent' or use_local_tts):
            if not (self.playback_thread and self.playback_thread.is_alive()):
                self.playback_thread = threading.Thread(target=self._audio_playback_worker, daemon=True)
                self.playback_thread.start()
                print("ðŸ”Š Audio playback thread started\n")

        # Prefer Deepgram Agent voice path; returns when agent thread exits
        threading.Thread(target=self._config_watcher, daemon=True).start()
        if str(self.cfg.get('voice_mode','agent')).lower() == 'agent':
            t = threading.Thread(target=self.run_agent_voice, name="agent_voice", daemon=True)
            t.start()
            try:
                while self.running and t.is_alive():
                    await asyncio.sleep(0.5)
            finally:
                return

        # Start config watcher (hot reload)
        threading.Thread(target=self._config_watcher, daemon=True).start()
        await self.connect_asr()
        microphone_task = asyncio.create_task(self.stream_microphone_input())
        events_task = asyncio.create_task(self.asr_receiver())

        try:
            await asyncio.gather(microphone_task, events_task)
        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ Shutting down AVA...")
            self.running = False
            # Signal playback thread to stop
            self.audio_queue.put(None)

    async def run(self):
        """Main run loop"""
        try:
            await self.start_conversation()
        finally:
            if self.asr_ws:
                await self.asr_ws.close()
            self.audio.terminate()

    async def cleanup(self):
        """Cleanup resources before reconnection"""
        self.running = False

        # Signal playback thread to stop
        if self.audio_queue:
            self.audio_queue.put(None)

        # Wait for playback thread
        if self.playback_thread and self.playback_thread.is_alive():
            self.playback_thread.join(timeout=2)

        # Close websocket
        if self.asr_ws:
            try:
                await self.asr_ws.close()
            except:
                pass

    # ---------------------- Identity & Self-awareness ----------------------
    def _load_identity(self) -> dict:
        try:
            if self.identity_path.exists():
                with open(self.identity_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception:
            pass
        # Defaults
        return {
            "name": "AVA",
            "developer": os.getenv('USERNAME') or os.getenv('USER') or 'User',
            "home": str(Path.home()),
            "location": str(Path(__file__).resolve().parent),
            "purpose": "Personal AI assistant that lives on this laptop.",
        }

    def _uptime_hms(self) -> str:
        dt = int(time.time() - self.started_at)
        h = dt // 3600; m = (dt % 3600) // 60; s = dt % 60
        return f"{h}h {m}m {s}s"

    def _self_status_text(self) -> str:
        idt = self.identity
        lines = []
        lines.append(f"I am {idt.get('name','AVA')}, your assistant developed by {idt.get('developer','you')}.")
        lines.append(f"I run locally on {platform.system()} {platform.release()} in {idt.get('location','my folder')}.")
        lines.append(f"Uptime {self._uptime_hms()}. Mic {MIC_RATE} Hz, TTS {PLAYBACK_RATE} Hz.")
        # Components
        asr_ok = bool(self.asr_ws and self.asr_ws.open)
        lines.append(f"ASR {'connected' if asr_ok else 'disconnected'}. TTS ready.")
        # Metrics
        m = self.metrics
        lines.append(f"ASR msgs {m.get('asr_messages',0)}, finals {m.get('asr_finals',0)}, TTS {m.get('tts_utterances',0)}, reconnects {m.get('reconnects',0)}.")
        le = m.get('last_error','')
        if le:
            lines.append(f"Last error: {le}")
        return " ".join(lines)

    def handle_self_modification(self, action: str, **kwargs) -> dict:
        """Handle self-modification requests
        
        Actions:
        - diagnose: Diagnose the codebase for issues
        - diagnose_error: Diagnose a specific error
        - analyze_file: Analyze a specific file
        - find_function: Find a function in a file
        - propose_fix: Propose a code fix (requires approval)
        - list_pending: List pending modifications
        - approve: Approve a modification
        - reject: Reject a modification
        - rollback: Rollback a modification
        - read_file: Read a core file
        - get_coding_knowledge: Get distilled coding knowledge
        - list_core_files: List all core AVA files
        """
        if not self.self_mod_enabled or not SELF_MOD_AVAILABLE:
            return {"status": "error", "message": "Self-modification system not available"}
        
        try:
            args = {"action": action, **kwargs}
            return self_mod_tool_handler(args)
        except Exception as e:
            return {"status": "error", "message": f"Self-mod error: {e}"}

    def handle_introspection(self, query_type: str = "full") -> dict:
        """Handle self-awareness and introspection queries
        
        Query types:
        - full: Complete self-knowledge dump
        - identity: Just identity info
        - capabilities: Available tools and their status
        - learning: Learned facts, corrections, patterns
        - diagnose: Run self-diagnosis
        - who_am_i: Dynamic self-description
        """
        if not self.self_awareness_enabled or not SELF_AWARENESS_AVAILABLE:
            return {"status": "error", "message": "Self-awareness system not available"}
        
        try:
            if query_type == "full":
                return {"status": "ok", "data": introspect()}
            elif query_type == "identity":
                return {"status": "ok", "data": self.self_awareness.get_identity()}
            elif query_type == "capabilities":
                return {"status": "ok", "data": {
                    "tools": self.self_awareness.get_available_tools(),
                    "status": self.self_awareness.get_tool_status()
                }}
            elif query_type == "learning":
                return {"status": "ok", "data": {
                    "facts": self.self_awareness.get_learned_facts(),
                    "corrections": self.self_awareness.get_corrections(),
                    "patterns": self.self_awareness.get_patterns(),
                    "preferences": self.self_awareness.get_preferences()
                }}
            elif query_type == "diagnose":
                return {"status": "ok", "data": self_diagnose()}
            elif query_type == "who_am_i":
                return {"status": "ok", "description": who_am_i()}
            elif query_type == "system":
                return {"status": "ok", "data": self.self_awareness.get_system_state()}
            else:
                return {"status": "error", "message": f"Unknown query type: {query_type}"}
        except Exception as e:
            return {"status": "error", "message": f"Introspection error: {e}"}

    def learn_correction(self, user_input: str, wrong: str, correct: str, context: str = "") -> dict:
        """Record a correction to learn from mistakes"""
        if not self.self_awareness_enabled or not SELF_AWARENESS_AVAILABLE:
            return {"status": "error", "message": "Self-awareness system not available"}
        
        try:
            success = learn_from_correction(user_input, wrong, correct, context)
            if success:
                return {"status": "ok", "message": "Correction recorded for future learning"}
            else:
                return {"status": "error", "message": "Failed to record correction"}
        except Exception as e:
            return {"status": "error", "message": f"Learning error: {e}"}

    def record_voice_interaction(self, transcript: str, response: str, helpful: bool = True) -> dict:
        """Record a voice interaction for passive learning"""
        if not self.passive_learning_enabled or not PASSIVE_LEARNING_AVAILABLE:
            return {"status": "skipped", "message": "Passive learning not available"}
        
        try:
            record_interaction(transcript, response, helpful)
            return {"status": "ok", "message": "Interaction recorded"}
        except Exception as e:
            return {"status": "error", "message": f"Recording error: {e}"}

    def get_passive_learning_status(self) -> dict:
        """Get passive learning status and summary"""
        if not self.passive_learning_enabled or not PASSIVE_LEARNING_AVAILABLE:
            return {"status": "disabled", "message": "Passive learning not available"}
        
        try:
            summary = get_learning_summary()
            context = get_passive_context()
            return {
                "status": "active",
                "current_context": context,
                "summary": summary
            }
        except Exception as e:
            return {"status": "error", "message": f"Status error: {e}"}

    def _detect_correction(self, transcript: str) -> bool:
        """Detect if the user is correcting AVA's last response"""
        if not self._last_ava_response:
            return False
        
        lower = transcript.lower().strip()
        
        # Check against correction patterns
        for pattern in self._correction_patterns:
            if re.match(pattern, lower, re.IGNORECASE):
                return True
        
        return False

    def _handle_correction(self, transcript: str) -> None:
        """Handle a detected correction - learn from the mistake"""
        if not self.self_awareness_enabled or not SELF_AWARENESS_AVAILABLE:
            return
        
        if not self._last_user_transcript or not self._last_ava_response:
            return
        
        try:
            # Record the correction for future learning
            learn_from_correction(
                user_input=self._last_user_transcript,
                wrong=self._last_ava_response[:200],  # Truncate for storage
                correct=transcript[:200],
                context=f"User corrected AVA's response"
            )
            print(f"[learning] Recorded correction: '{self._last_user_transcript[:30]}...' â†’ correction noted")
        except Exception as e:
            print(f"[learning] Error recording correction: {e}")

    def _check_past_mistakes(self, transcript: str) -> str:
        """Check if we've made a similar mistake before and get guidance"""
        if not self.self_awareness_enabled or not SELF_AWARENESS_AVAILABLE:
            return ""
        
        try:
            similar = check_past_mistakes(transcript)
            if similar:
                # Return guidance to avoid repeating the mistake
                wrong = similar.get('wrong', '')
                correct = similar.get('correct', '')
                if wrong and correct:
                    return f" NOTE: For similar requests, user previously corrected: don't say '{wrong[:50]}', instead '{correct[:50]}'. "
        except Exception as e:
            pass
        
        return ""

    def _get_enhanced_transcript(self, transcript: str) -> str:
        """Enhance transcript with past mistake context if relevant"""
        guidance = self._check_past_mistakes(transcript)
        if guidance:
            return f"{transcript} [SYSTEM GUIDANCE:{guidance}]"
        return transcript

    async def _maybe_handle_local_intent(self, transcript: str) -> bool:
        """Handle local intents WITHOUT speaking in agent mode (agent will speak)"""
        # In agent mode, we DON'T want to speak locally - just return False
        # to let the agent handle everything via native conversation
        # This prevents double-speaking (local TTS + agent TTS)

        # DISABLED: Local intent handling in agent mode causes double responses
        # The agent should handle all self-awareness questions natively
        return False


async def main():
    """Entry point with auto-restart"""
    reconnect_count = 0

    while True:
        try:
            print("\n" + "=" * 80)
            if reconnect_count > 0:
                print(f"ðŸ”„ Reconnecting to AVA (attempt #{reconnect_count + 1})...")
            print("=" * 80 + "\n")

            ava = StandaloneRealtimeAVA()
            await ava.run()

            # If we get here without exception, it was a clean shutdown
            break

        except websockets.exceptions.ConnectionClosedOK as e:
            # Session expired (60 minute limit)
            print("\n" + "=" * 80)
            print("â° Session expired (60-minute limit reached)")
            print("ðŸ”„ Auto-restarting AVA in 3 seconds...")
            print("=" * 80)
            reconnect_count += 1

            # Cleanup
            try:
                await ava.cleanup()
            except:
                pass

            # Wait before reconnecting
            await asyncio.sleep(3)

        except websockets.exceptions.ConnectionClosedError as e:
            # Connection lost unexpectedly
            print("\n" + "=" * 80)
            print(f"âš ï¸  Connection lost: {e}")
            print("ðŸ”„ Auto-restarting AVA in 5 seconds...")
            print("=" * 80)
            reconnect_count += 1

            # Cleanup
            try:
                await ava.cleanup()
            except:
                pass

            # Wait before reconnecting
            await asyncio.sleep(5)

        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ Shutting down AVA...")
            try:
                await ava.cleanup()
            except:
                pass
            break

        except Exception as e:
            print(f"\nâŒ Unexpected error: {e}")
            print("ðŸ”„ Attempting to restart in 10 seconds...")
            reconnect_count += 1

            try:
                await ava.cleanup()
            except:
                pass

            await asyncio.sleep(10)
    print("\n" + "=" * 80)
    print("AVA Standalone ended")
    print("=" * 80)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nðŸ‘‹ Goodbye!")
